{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Multiply, Add, Activation, LSTM,Input, Concatenate, Flatten,Reshape, BatchNormalization, Masking, Conv1D,MaxPooling1D, Dropout, concatenate, Permute,RepeatVector, merge\n",
    "from keras.models import Model\n",
    "\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "- Clean Up N/As \n",
    "- History and Future Lag (own Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MU'][:10] = None\n",
    "\n",
    "data['MU'][15] = None\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.interpolate(limit_direction='both', limit_area='inside')\n",
    "\n",
    "data = data.fillna(0)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns:\n",
    "    scaler = []\n",
    "    if c != 'Date':\n",
    "        sc = MinMaxScaler()\n",
    "        scaler.append(sc.fit(data[[c]]))\n",
    "        data[c] = sc.transform(data[[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformData(data, pastlag, futurelag = 1, validation_span = 16, arraylength = None):\n",
    "\n",
    "    if arraylength is None:\n",
    "        arraylength = pastlag\n",
    "    cols = []\n",
    "    past_train = []\n",
    "    future_train  = []\n",
    "    past_validate = []\n",
    "    future_validate  = []\n",
    "    for c in data.columns:\n",
    "        if c != 'Date':\n",
    "            cols.append(c)\n",
    "\n",
    "    for c in cols:    \n",
    "        ar = np.asarray(data[c])\n",
    "        l = len(ar)\n",
    "        iv = (l-futurelag) - validation_span\n",
    "        #iv = pastlag + int(((l-futurelag)-pastlag)*(1-validation_percentage))\n",
    "        for i in range(pastlag,len(ar)-futurelag + 1):\n",
    "            if i <= iv:\n",
    "                for j in range(futurelag):\n",
    "                    if i-arraylength < 0:\n",
    "                        p_ar = ar[0:i]\n",
    "                        p_ar = np.pad(p_ar,(arraylength - len(p_ar),0),'constant')\n",
    "                    else:\n",
    "                        p_ar = ar[i-arraylength:i]\n",
    "                    past_train.append(p_ar)\n",
    "                    #future_train.append(ar[i:i+futurelag])\n",
    "                    future_train.append(ar[i+j:i+j+1])\n",
    "            else:\n",
    "                for j in range(futurelag):\n",
    "                    if i-arraylength < 0:\n",
    "                        p_ar = ar[0:i]\n",
    "                        p_ar = np.pad(p_ar,(arraylength - len(p_ar),0),'constant')\n",
    "                    else:\n",
    "                        p_ar = ar[i-arraylength:i]\n",
    "                    past_validate.append(p_ar)\n",
    "                    future_validate.append(ar[i+j:i+j+1])\n",
    "                    #future_validate.append(ar[i:i+futurelag])\n",
    "    return np.asarray(past_train), np.asarray(future_train), np.asarray(past_validate), np.asarray(future_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformDataByCols(data, pastlag, targetCol, ExoCols = [], futurelag = 1, validation_span = 16, arraylength = None):\n",
    "\n",
    "    if arraylength is None:\n",
    "        arraylength = pastlag\n",
    "    past_train = []\n",
    "    future_train  = []\n",
    "    past_validate = []\n",
    "    future_validate  = []\n",
    "    \n",
    "    cols = [targetCol] + ExoCols\n",
    "    for c in cols: \n",
    "        p_t_c =  []\n",
    "        f_t_c = []\n",
    "        p_v_c = []\n",
    "        f_v_c  = []\n",
    "        ar = np.asarray(data[c])\n",
    "        l = len(ar)\n",
    "        iv = (l-futurelag) - validation_span\n",
    "        #iv = pastlag + int(((l-futurelag)-pastlag)*(1-validation_percentage))\n",
    "        for i in range(pastlag,len(ar)-futurelag+1):\n",
    "            if i <= iv:\n",
    "                if i-arraylength < 0:\n",
    "                        p_ar = ar[0:i]\n",
    "                        p_ar = np.pad(p_ar,(arraylength - len(p_ar),0),'constant')\n",
    "                else:\n",
    "                        p_ar = ar[i-arraylength:i]\n",
    "                p_t_c.append(p_ar)\n",
    "                f_t_c.append(ar[i:i+futurelag])\n",
    "            else:\n",
    "                if i-arraylength < 0:\n",
    "                        p_ar = ar[0:i]\n",
    "                        p_ar = np.pad(p_ar,(arraylength - len(p_ar),0),'constant')\n",
    "                else:\n",
    "                        p_ar = ar[i-arraylength:i]\n",
    "                p_v_c.append(p_ar)\n",
    "                f_v_c.append(ar[i:i+futurelag])\n",
    "        \n",
    "        past_train.append(np.expand_dims(np.asarray(p_t_c),-1))\n",
    "        past_validate.append(np.expand_dims(np.asarray(p_v_c),-1))\n",
    "        if  c == targetCol:\n",
    "            future_train = np.asarray(f_t_c)\n",
    "            future_validate = np.asarray(f_v_c)\n",
    "                \n",
    "                \n",
    "    return past_train, future_train, past_validate, future_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "def RMSEperLag(x_true,x_pred):\n",
    "    return np.sqrt(np.mean((x_true - x_pred)**2,axis= 0))\n",
    "\n",
    "\n",
    "\n",
    "class IntervalEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=10):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        \n",
    "    def on_epoch_start(self,epoch, logs={}):\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            \n",
    "            \n",
    "            score = RMSEperLag(self.y_val, y_pred)\n",
    "            \n",
    "            for s in score:\n",
    "                print(s)\n",
    "            #print(\"interval evaluation - epoch: {:d} - score: {:.6f}\".format(epoch, score))\n",
    "\n",
    "    \n",
    "class LRRestart(Callback):\n",
    "    def __init__(self, maxLR, maxEpoch, patience, minLR = 0.1e-5):\n",
    "        super(Callback, self).__init__()\n",
    "        self.maxLR = maxLR    \n",
    "        self.patience = patience\n",
    "        self.minLR = minLR\n",
    "        self.restart = True\n",
    "        self.lastRestartEpoch = 1\n",
    "        self.best_val_loss = None\n",
    "        self.wait = 0 \n",
    "        \n",
    "    def schedule(epoch):\n",
    "        reductionRate = ((self.maxLR - self.MinLR)/self.maxEpoch) / self.maxLR\n",
    "        lr = self.maxLR - (epoch - lastRestartEpoch) *max(reductionRate*self.maxLR, self.minLR)\n",
    "        return lr\n",
    "    \n",
    "    def on_training_begin(self, logs ={}):\n",
    "        \n",
    "        self.wait = 0\n",
    "        \n",
    "    def on_epoch_start(self,epoch, logs=None):\n",
    "            \n",
    "        if self.restart or (epoch - self.lastRestartEpoch) > self.maxEpoch:\n",
    "            self.lastRestartEpoch = epoch\n",
    "            self.restart = False\n",
    "        lr = schedule(epoch)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "    def on_epoch_end(self, epoch,logs = None):\n",
    "            \n",
    "            logs = logs or {}\n",
    "            logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
    "            \n",
    "            self.current_val_loss = logs.get('val_loss')\n",
    "            \n",
    "            if self.best_val_loss is None:\n",
    "                self.best_val_loss = self.current_val_loss\n",
    "            \n",
    "            if self.current_val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = self.current_val_loss \n",
    "                self.wait = 0\n",
    "            else:\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.patience:\n",
    "                    self.restart = True\n",
    "                    #self.model.stop_training = True\n",
    "                    \n",
    "class UnfreezeLayer(Callback):\n",
    "    def __init__(self, layerNames = [], unfreezeSchedule = []):\n",
    "        super(Callback, self).__init__()\n",
    "        self.layerNames = layerNames    \n",
    "        self.unfreezeSchedule = unfreezeSchedule\n",
    "        \n",
    "    def on_epoch_start(self,epoch, logs=None):\n",
    "        \n",
    "        for i, u in enumerate(self.unfreezeSchedule):\n",
    "            if u == epoch:\n",
    "                for ln in  [l.name for l in g_lstm.layers if layerNames[u] + '.' in l.name ]:\n",
    "                    self.model.get_layer(ln).trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "\n",
    "def PermaDropout(rate):\n",
    "    return Lambda(lambda x: K.dropout(x, level=rate))\n",
    "\n",
    "def expand_dims(x):\n",
    "    return K.expand_dims(x,1)\n",
    "def expand_dims_output_shape(input_shape):\n",
    "    return (input_shape[0],1,input_shape[1])\n",
    "\n",
    "def expand_dims2(x):\n",
    "    return K.expand_dims(x,2)\n",
    "def expand_dims_output_shape2(input_shape):\n",
    "    return (input_shape[0],1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_t, future_t, past_v, future_v  = transformData(data,pastlag = 90, futurelag = 1, arraylength   = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General LSTM Model\n",
    "\n",
    "- define Model\n",
    "- Train Model on all Data \n",
    "- save trained weights\n",
    "\n",
    "\n",
    "layer_conv_1d(filters=64, kernel_size=4, activation=\"relu\", input_shape=c(lookback, dim(dm)[[-1]])) %>%\n",
    "  layer_max_pooling_1d(pool_size=4) %>%\n",
    "  layer_flatten() %>%\n",
    "  layer_dense(units=lookback * dim(dm)[[-1]], activation=\"relu\") %>%\n",
    "  layer_dropout(rate=0.2) %>%\n",
    "  layer_dense(units=1, activation=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DECODER MODEL\n",
    "decoder = Sequential()\n",
    "dec_input = Input((90,1))\n",
    "#dec_batchNorm = BatchNormalization()(dec_input)\n",
    "dec_batchNorm = dec_input\n",
    "### Conv\n",
    "\n",
    "dec_conv = Conv1D(8,3,activation=\"relu\")(dec_batchNorm)\n",
    "dec_pool = MaxPooling1D(2)(dec_conv)\n",
    "dec_conv2 = Conv1D(4,5,activation=\"relu\")(dec_pool)\n",
    "dec_pool2 = MaxPooling1D(4)(dec_conv)\n",
    "#dec_pool2 = SeqSelfAttention(attention_activation='sigmoid')(dec_pool2) \n",
    "#dec_lstm_conv = LSTM(10,dropout = 0.5 ,return_sequences = False)(dec_pool2)\n",
    "dec_conv_flat = Flatten()(dec_pool2)\n",
    "#dec_conv_flat = Lambda(lambda x: K.batch_flatten(x))(dec_pool2)\n",
    "dec_conv_flat = Dropout(0.2)(dec_conv_flat)\n",
    "#dec_conv_flat = dec_lstm_conv\n",
    "\n",
    "### LSTM \n",
    "#dec_attention = SeqSelfAttention(attention_activation='sigmoid')(dec_batchNorm) \n",
    "dec_lstm = LSTM(10,dropout = 0.5 ,return_sequences = False)(dec_batchNorm)\n",
    "#dec_attention = SeqSelfAttention(attention_activation='sigmoid')(dec_lstm) \n",
    "#dec_lstm_flat = Flatten()(dec_attention)\n",
    "dec_lstm_flat = dec_lstm\n",
    "\n",
    "\n",
    "dec_output = concatenate([dec_conv_flat, dec_lstm_flat],axis = -1)\n",
    "#dec_output = dec_lstm_flat\n",
    "#dec_output = dec_conv_flat\n",
    "\n",
    "decoder = Model(dec_input,dec_output)\n",
    "\n",
    "g_lstm = Sequential()\n",
    "g_lstm.add(decoder)\n",
    "#g_lstm.add(Dense(90*4, activation=\"relu\"))\n",
    "#g_lstm.add(Dropout(0.2))\n",
    "#g_lstm.add(Dense(30, name = 'glstm_dense'))\n",
    "g_lstm.add(Dense(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g_lstm.compile(loss = 'mse',optimizer = 'adam')\n",
    "ival = IntervalEvaluation(validation_data =(np.expand_dims(past_v,-1),future_v), interval=1)\n",
    "lrre = LRRestart(maxLR = .5, maxEpoch = 10, patience = 5)\n",
    "\n",
    "g_lstm.fit(np.expand_dims(past_t,-1), future_t, batch_size = 200, epochs=100, validation_data = (np.expand_dims(past_v,-1),future_v), \n",
    "           #callbacks=[lrre],\n",
    "           verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lstm.save('glstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predA = decoder.predict(np.expand_dims(past_t,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate with Exogenen Variables\n",
    "\n",
    "- Embedding of  each TS with general  LSTM\n",
    "- Multi Attettion Head over all TS\n",
    "- Estimate Full future timespan\n",
    "\n",
    "Train Model first with froozen LSTM Layer... unfreeze later in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EXO = [\"BAC\", \"MU\",\"AKS\",\"CLF\",\"SKX\",\"PBR\",\"NFX\",\"MS\",\"CTL\"]\n",
    "past_t, future_t, past_v, future_v  = transformDataByCols(data,targetCol = \"ET\", ExoCols = EXO, pastlag = 90, futurelag = 10, arraylength =90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs,TIME_STEPS, SINGLE_ATTENTION_VECTOR ):\n",
    "\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "\n",
    "    input_dim =186# int(inputs.shape[2])\n",
    "\n",
    "    a = Permute((2, 1))(inputs)\n",
    "\n",
    "    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
    "\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])#, name='attention_mul', mode='mul')\n",
    "\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense ,LSTM,concatenate\n",
    "\n",
    "STEPS = 2\n",
    "\n",
    "\n",
    "in_in = []\n",
    "out_in = []\n",
    "\n",
    "for i in range(len(EXO)+1):\n",
    "    \n",
    "    \n",
    "    inp = Input((90,1))\n",
    "    in_in.append(inp)\n",
    "        \n",
    "    inpp = inp \n",
    "    out_pp = []\n",
    "    for i in range(STEPS):\n",
    "        out_temp = g_lstm(inpp)\n",
    "        \n",
    "        inpp = concatenate ([inpp[:, 1:],Lambda(expand_dims2, expand_dims_output_shape2)(out_temp)], axis=1)\n",
    "        \n",
    "        out_pp.append(out_temp)\n",
    "        \n",
    "    a = concatenate(out_pp,axis=-1)\n",
    "    a= Lambda(expand_dims, expand_dims_output_shape)(a)\n",
    "    out_in.append(a)\n",
    "        \n",
    "out_dec=concatenate(out_in,axis=1)   \n",
    "#out = SeqSelfAttention(attention_activation='sigmoid')(out_dec)\n",
    "    \n",
    "o2 = Flatten()(out_dec)\n",
    "o2 = Dense(STEPS)(o2)\n",
    "model = Model(inputs = in_in, outputs = [o2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense ,LSTM,concatenate\n",
    "\n",
    "\n",
    "in_in = []\n",
    "out_in = []\n",
    "\n",
    "#bb = BatchNormalization()\n",
    "\n",
    "\n",
    "decoder.trainable = False\n",
    "\n",
    "#a_layer = SeqSelfAttention(attention_activation='sigmoid', name = 'att.0', weights = glstm_a_weights)\n",
    "#a_layer.trainable = False\n",
    "for i in range(len(EXO)+1):\n",
    "    inp = Input((90,1))\n",
    "    in_in.append(inp)  \n",
    "    a = decoder(inp)\n",
    "    #a = concatenate([a,Flatten()(inp)],axis=-1)\n",
    "    a= Lambda(expand_dims, expand_dims_output_shape)(a)\n",
    "    out_in.append(a)\n",
    "        \n",
    "\n",
    "if len(EXO) >= 1:\n",
    "    out_dec=concatenate(out_in,axis=1)\n",
    "else:\n",
    "    out_dec = out_in\n",
    "    \n",
    "#out_dec = BatchNormalization()(out_dec)\n",
    "out_ar = []    \n",
    "\n",
    "#out_dec = MultiHeadAttention(head_num=5)(out_dec)\n",
    "\n",
    "#out = attention_3d_block(out_dec, len(EXO) + 1, False)\n",
    "out = SeqSelfAttention(attention_activation='sigmoid')(out_dec)\n",
    "'''\n",
    "for  j in range(16):\n",
    "    #out = attention_3d_block(out_dec, len(EXO) + 1, True)\n",
    "    #out = SeqSelfAttention(attention_activation='sigmoid')(out_dec)\n",
    "\n",
    "    #out = MultiHeadAttention(\n",
    "    #    head_num=5)(out_dec)\n",
    "    out= Dropout(0.2)(out_dec)\n",
    "    out = Flatten()(out)\n",
    "    out = Dense(20, activation = \"relu\")(out)\n",
    "    out= Dropout(0.2)(out)\n",
    "\n",
    "#out = Dense(30)(out)\n",
    "    out = Dense(1)(out)\n",
    "    out_ar.append(out)\n",
    "    \n",
    "oo = concatenate(out_ar,axis = -1)\n",
    "'''\n",
    "#o2 = attention_3d_block(out_dec, len(EXO), False)\n",
    "o2 = Flatten()(out_dec)\n",
    "o2 = Dense(10)(o2)\n",
    "#ooo = Add()([oo,o2])\n",
    "model = Model(inputs = in_in, outputs = [o2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse(yTrue,yPred):\n",
    "    ones = K.ones_like(yTrue[0,:])\n",
    "    idx = K.cumsum(ones)\n",
    "    \n",
    "    return K.mean((1/idx)*K.square(yTrue- yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mse',optimizer = 'adam')\n",
    "\n",
    "#ival = IntervalEvaluation(validation_data =(past_v,future_v), interval=1)\n",
    "lrre = LRRestart(maxLR = .5, maxEpoch = 10, patience = 5)\n",
    "unfreeze = UnfreezeLayer(['lstm'], [20])\n",
    "\n",
    "model.fit(past_t, future_t, batch_size = 32, epochs=100, validation_data = (past_v,future_v))#, callbacks = [lrre,unfreeze])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predA = model.predict(past_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "I =-1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "past= np.squeeze(past_v[0][I])\n",
    "future= future_v[I]\n",
    "past_x =[i for i in range(len(past))]\n",
    "future_x =[i for i in range(len(past),len(past)+len(future))]\n",
    "pred = predA[I]\n",
    "\n",
    "\n",
    "#pop_india = [449.48, 553.57, 696.783, 870.133, 1000.4, 1309.1]\n",
    "plt.plot(past_x, past, color='blue')\n",
    "plt.plot(future_x, future, color='g')\n",
    "plt.plot(future_x, pred, color='orange')\n",
    "plt.xlabel('Countries')\n",
    "plt.ylabel('Population in million')\n",
    "plt.title('Pakistan India Population till 2010')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_t, future_t, past_v, future_v  = transformDataByCols(data,targetCol = \"IWM\", ExoCols = [], pastlag = 30)\n",
    "t_lstm = Sequential()\n",
    "\n",
    "t_lstm.add(LSTM(20, dropout=0.2, name = 'glstm'))\n",
    "t_lstm.add(Dense(1, name = 'glstm_dense'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_lstm.compile(loss = 'mse',optimizer = 'adam')\n",
    "t_lstm.fit(past_t, future_t, batch_size = 10, epochs=100, validation_data = (past_v,future_v), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
