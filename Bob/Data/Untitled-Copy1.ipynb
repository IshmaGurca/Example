{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Multiply, Add, Activation, LSTM,Input, Concatenate, Flatten,Reshape, BatchNormalization, Masking, Conv1D,MaxPooling1D, Dropout, concatenate, Permute,RepeatVector, merge\n",
    "from keras.models import Model\n",
    "\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "- Clean Up N/As \n",
    "- History and Future Lag (own Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Local\\conda\\conda\\envs\\Time\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\tobia\\AppData\\Local\\conda\\conda\\envs\\Time\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AKS</th>\n",
       "      <th>BAC</th>\n",
       "      <th>CLF</th>\n",
       "      <th>CTL</th>\n",
       "      <th>ET</th>\n",
       "      <th>MS</th>\n",
       "      <th>MU</th>\n",
       "      <th>NFX</th>\n",
       "      <th>PBR</th>\n",
       "      <th>SKX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>10.54</td>\n",
       "      <td>21.779140</td>\n",
       "      <td>8.555229</td>\n",
       "      <td>20.231129</td>\n",
       "      <td>16.380995</td>\n",
       "      <td>41.021313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.160000</td>\n",
       "      <td>10.724895</td>\n",
       "      <td>24.889999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>11.11</td>\n",
       "      <td>22.185143</td>\n",
       "      <td>8.803638</td>\n",
       "      <td>20.334877</td>\n",
       "      <td>16.722797</td>\n",
       "      <td>41.564457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.490002</td>\n",
       "      <td>10.695187</td>\n",
       "      <td>25.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>10.88</td>\n",
       "      <td>21.924143</td>\n",
       "      <td>8.664530</td>\n",
       "      <td>20.350840</td>\n",
       "      <td>16.731340</td>\n",
       "      <td>41.183304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.980000</td>\n",
       "      <td>10.992275</td>\n",
       "      <td>25.219999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>10.72</td>\n",
       "      <td>21.924143</td>\n",
       "      <td>8.416120</td>\n",
       "      <td>20.255070</td>\n",
       "      <td>16.970608</td>\n",
       "      <td>41.783615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.970001</td>\n",
       "      <td>10.764507</td>\n",
       "      <td>25.030001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>10.31</td>\n",
       "      <td>21.798471</td>\n",
       "      <td>8.376374</td>\n",
       "      <td>20.135357</td>\n",
       "      <td>16.167364</td>\n",
       "      <td>40.697342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.320000</td>\n",
       "      <td>10.724895</td>\n",
       "      <td>25.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>11.05</td>\n",
       "      <td>22.175476</td>\n",
       "      <td>9.330268</td>\n",
       "      <td>20.111420</td>\n",
       "      <td>16.013552</td>\n",
       "      <td>41.030857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.610001</td>\n",
       "      <td>10.913053</td>\n",
       "      <td>25.620001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>11.02</td>\n",
       "      <td>22.301144</td>\n",
       "      <td>9.727723</td>\n",
       "      <td>20.143341</td>\n",
       "      <td>16.090458</td>\n",
       "      <td>41.612099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.150002</td>\n",
       "      <td>11.428003</td>\n",
       "      <td>25.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>9.87</td>\n",
       "      <td>22.156145</td>\n",
       "      <td>9.469377</td>\n",
       "      <td>20.095455</td>\n",
       "      <td>16.064823</td>\n",
       "      <td>41.573994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>11.378489</td>\n",
       "      <td>25.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>9.71</td>\n",
       "      <td>22.243143</td>\n",
       "      <td>9.171285</td>\n",
       "      <td>20.159302</td>\n",
       "      <td>15.893919</td>\n",
       "      <td>41.745514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.900002</td>\n",
       "      <td>11.210139</td>\n",
       "      <td>25.459999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>9.45</td>\n",
       "      <td>21.315136</td>\n",
       "      <td>9.101730</td>\n",
       "      <td>20.326895</td>\n",
       "      <td>15.842651</td>\n",
       "      <td>40.163727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.110001</td>\n",
       "      <td>11.160625</td>\n",
       "      <td>25.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-01-18</td>\n",
       "      <td>10.07</td>\n",
       "      <td>21.875805</td>\n",
       "      <td>9.370013</td>\n",
       "      <td>20.326895</td>\n",
       "      <td>15.381215</td>\n",
       "      <td>40.840271</td>\n",
       "      <td>22.320000</td>\n",
       "      <td>41.669998</td>\n",
       "      <td>11.061595</td>\n",
       "      <td>25.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017-01-19</td>\n",
       "      <td>9.52</td>\n",
       "      <td>21.779140</td>\n",
       "      <td>8.952684</td>\n",
       "      <td>20.350840</td>\n",
       "      <td>15.133404</td>\n",
       "      <td>40.449596</td>\n",
       "      <td>21.709999</td>\n",
       "      <td>41.619999</td>\n",
       "      <td>11.031886</td>\n",
       "      <td>24.549999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-01-20</td>\n",
       "      <td>9.52</td>\n",
       "      <td>21.885473</td>\n",
       "      <td>8.674465</td>\n",
       "      <td>20.398720</td>\n",
       "      <td>15.398306</td>\n",
       "      <td>40.583000</td>\n",
       "      <td>21.959999</td>\n",
       "      <td>41.820000</td>\n",
       "      <td>11.091304</td>\n",
       "      <td>24.969999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>9.29</td>\n",
       "      <td>21.808140</td>\n",
       "      <td>8.724148</td>\n",
       "      <td>20.598238</td>\n",
       "      <td>15.594842</td>\n",
       "      <td>39.982689</td>\n",
       "      <td>21.889999</td>\n",
       "      <td>41.160000</td>\n",
       "      <td>11.220043</td>\n",
       "      <td>24.860001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-01-24</td>\n",
       "      <td>8.68</td>\n",
       "      <td>22.185143</td>\n",
       "      <td>9.230904</td>\n",
       "      <td>20.406704</td>\n",
       "      <td>15.911013</td>\n",
       "      <td>40.716400</td>\n",
       "      <td>22.850000</td>\n",
       "      <td>42.419998</td>\n",
       "      <td>10.992275</td>\n",
       "      <td>25.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-01-25</td>\n",
       "      <td>8.26</td>\n",
       "      <td>22.591145</td>\n",
       "      <td>8.912939</td>\n",
       "      <td>20.502470</td>\n",
       "      <td>16.167364</td>\n",
       "      <td>41.793148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>10.863537</td>\n",
       "      <td>25.780001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017-01-26</td>\n",
       "      <td>8.19</td>\n",
       "      <td>22.658810</td>\n",
       "      <td>8.813575</td>\n",
       "      <td>20.797758</td>\n",
       "      <td>16.201542</td>\n",
       "      <td>41.945610</td>\n",
       "      <td>23.520000</td>\n",
       "      <td>42.160000</td>\n",
       "      <td>10.804118</td>\n",
       "      <td>24.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>8.07</td>\n",
       "      <td>22.581478</td>\n",
       "      <td>8.962622</td>\n",
       "      <td>20.717951</td>\n",
       "      <td>15.859742</td>\n",
       "      <td>41.782875</td>\n",
       "      <td>23.969999</td>\n",
       "      <td>41.779999</td>\n",
       "      <td>10.685283</td>\n",
       "      <td>24.969999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017-01-30</td>\n",
       "      <td>8.09</td>\n",
       "      <td>22.185143</td>\n",
       "      <td>8.614847</td>\n",
       "      <td>20.598238</td>\n",
       "      <td>15.398306</td>\n",
       "      <td>41.275551</td>\n",
       "      <td>24.180000</td>\n",
       "      <td>40.119999</td>\n",
       "      <td>10.289166</td>\n",
       "      <td>24.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>8.08</td>\n",
       "      <td>21.885473</td>\n",
       "      <td>8.714211</td>\n",
       "      <td>20.638147</td>\n",
       "      <td>15.338489</td>\n",
       "      <td>40.672504</td>\n",
       "      <td>24.110001</td>\n",
       "      <td>40.080002</td>\n",
       "      <td>10.160426</td>\n",
       "      <td>25.120001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>7.79</td>\n",
       "      <td>22.127142</td>\n",
       "      <td>8.724148</td>\n",
       "      <td>20.358822</td>\n",
       "      <td>15.458119</td>\n",
       "      <td>40.873516</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>39.889999</td>\n",
       "      <td>10.130718</td>\n",
       "      <td>24.290001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017-02-02</td>\n",
       "      <td>7.92</td>\n",
       "      <td>21.962805</td>\n",
       "      <td>9.141476</td>\n",
       "      <td>20.159302</td>\n",
       "      <td>15.817014</td>\n",
       "      <td>40.327904</td>\n",
       "      <td>24.790001</td>\n",
       "      <td>40.509998</td>\n",
       "      <td>10.051494</td>\n",
       "      <td>23.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017-02-03</td>\n",
       "      <td>8.13</td>\n",
       "      <td>22.513815</td>\n",
       "      <td>8.734084</td>\n",
       "      <td>20.286993</td>\n",
       "      <td>16.533009</td>\n",
       "      <td>42.529518</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>41.630001</td>\n",
       "      <td>10.229747</td>\n",
       "      <td>23.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>8.20</td>\n",
       "      <td>22.349480</td>\n",
       "      <td>9.042112</td>\n",
       "      <td>19.991703</td>\n",
       "      <td>16.090397</td>\n",
       "      <td>42.720959</td>\n",
       "      <td>24.340000</td>\n",
       "      <td>40.709999</td>\n",
       "      <td>10.091105</td>\n",
       "      <td>23.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2017-02-07</td>\n",
       "      <td>8.01</td>\n",
       "      <td>22.136806</td>\n",
       "      <td>9.061985</td>\n",
       "      <td>19.856033</td>\n",
       "      <td>16.533009</td>\n",
       "      <td>42.950695</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>39.020000</td>\n",
       "      <td>9.982173</td>\n",
       "      <td>22.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2017-02-08</td>\n",
       "      <td>8.04</td>\n",
       "      <td>21.914476</td>\n",
       "      <td>9.459441</td>\n",
       "      <td>19.496901</td>\n",
       "      <td>16.706587</td>\n",
       "      <td>42.031757</td>\n",
       "      <td>24.209999</td>\n",
       "      <td>39.560001</td>\n",
       "      <td>10.140620</td>\n",
       "      <td>22.540001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017-02-09</td>\n",
       "      <td>8.66</td>\n",
       "      <td>22.349480</td>\n",
       "      <td>11.297672</td>\n",
       "      <td>19.488920</td>\n",
       "      <td>16.619799</td>\n",
       "      <td>42.931549</td>\n",
       "      <td>24.450001</td>\n",
       "      <td>40.910000</td>\n",
       "      <td>10.041593</td>\n",
       "      <td>23.280001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017-02-10</td>\n",
       "      <td>8.39</td>\n",
       "      <td>22.310810</td>\n",
       "      <td>11.079071</td>\n",
       "      <td>19.544785</td>\n",
       "      <td>16.350758</td>\n",
       "      <td>42.787968</td>\n",
       "      <td>24.049999</td>\n",
       "      <td>41.770000</td>\n",
       "      <td>10.318873</td>\n",
       "      <td>27.780001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017-02-13</td>\n",
       "      <td>8.76</td>\n",
       "      <td>22.620148</td>\n",
       "      <td>11.844173</td>\n",
       "      <td>19.648535</td>\n",
       "      <td>16.125111</td>\n",
       "      <td>43.324013</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>10.467419</td>\n",
       "      <td>26.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017-02-14</td>\n",
       "      <td>8.57</td>\n",
       "      <td>23.258148</td>\n",
       "      <td>11.953473</td>\n",
       "      <td>19.584692</td>\n",
       "      <td>16.819407</td>\n",
       "      <td>44.070652</td>\n",
       "      <td>23.120001</td>\n",
       "      <td>43.669998</td>\n",
       "      <td>10.804118</td>\n",
       "      <td>26.639999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>4.39</td>\n",
       "      <td>26.165396</td>\n",
       "      <td>5.981705</td>\n",
       "      <td>12.640324</td>\n",
       "      <td>15.167289</td>\n",
       "      <td>47.211277</td>\n",
       "      <td>46.180000</td>\n",
       "      <td>29.440001</td>\n",
       "      <td>9.873241</td>\n",
       "      <td>32.799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>2017-11-17</td>\n",
       "      <td>4.51</td>\n",
       "      <td>26.028505</td>\n",
       "      <td>6.091005</td>\n",
       "      <td>12.683171</td>\n",
       "      <td>14.866495</td>\n",
       "      <td>47.308418</td>\n",
       "      <td>46.160000</td>\n",
       "      <td>30.299999</td>\n",
       "      <td>10.041593</td>\n",
       "      <td>33.619999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2017-11-20</td>\n",
       "      <td>4.63</td>\n",
       "      <td>26.145838</td>\n",
       "      <td>6.130751</td>\n",
       "      <td>12.683171</td>\n",
       "      <td>14.748001</td>\n",
       "      <td>47.706711</td>\n",
       "      <td>47.639999</td>\n",
       "      <td>30.010000</td>\n",
       "      <td>10.071300</td>\n",
       "      <td>33.709999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>4.85</td>\n",
       "      <td>26.136061</td>\n",
       "      <td>6.259924</td>\n",
       "      <td>12.451789</td>\n",
       "      <td>14.629505</td>\n",
       "      <td>47.968994</td>\n",
       "      <td>49.400002</td>\n",
       "      <td>30.180000</td>\n",
       "      <td>9.932658</td>\n",
       "      <td>33.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2017-11-22</td>\n",
       "      <td>4.88</td>\n",
       "      <td>26.067616</td>\n",
       "      <td>6.339415</td>\n",
       "      <td>12.288965</td>\n",
       "      <td>14.738886</td>\n",
       "      <td>47.687283</td>\n",
       "      <td>49.139999</td>\n",
       "      <td>30.660000</td>\n",
       "      <td>10.160426</td>\n",
       "      <td>33.669998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2017-11-24</td>\n",
       "      <td>4.88</td>\n",
       "      <td>25.999172</td>\n",
       "      <td>6.567952</td>\n",
       "      <td>12.324585</td>\n",
       "      <td>14.793575</td>\n",
       "      <td>47.658138</td>\n",
       "      <td>49.680000</td>\n",
       "      <td>30.740000</td>\n",
       "      <td>10.219845</td>\n",
       "      <td>33.540001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>4.71</td>\n",
       "      <td>25.999172</td>\n",
       "      <td>6.339415</td>\n",
       "      <td>12.128674</td>\n",
       "      <td>14.675081</td>\n",
       "      <td>47.599854</td>\n",
       "      <td>48.049999</td>\n",
       "      <td>29.690001</td>\n",
       "      <td>10.021786</td>\n",
       "      <td>34.689999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>4.79</td>\n",
       "      <td>27.025839</td>\n",
       "      <td>6.577888</td>\n",
       "      <td>12.191010</td>\n",
       "      <td>14.638620</td>\n",
       "      <td>48.959843</td>\n",
       "      <td>47.930000</td>\n",
       "      <td>29.920000</td>\n",
       "      <td>10.061398</td>\n",
       "      <td>34.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>4.89</td>\n",
       "      <td>27.651619</td>\n",
       "      <td>6.369224</td>\n",
       "      <td>12.947938</td>\n",
       "      <td>14.392516</td>\n",
       "      <td>49.853554</td>\n",
       "      <td>43.740002</td>\n",
       "      <td>30.100000</td>\n",
       "      <td>9.714794</td>\n",
       "      <td>34.560001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>4.87</td>\n",
       "      <td>27.661440</td>\n",
       "      <td>6.617633</td>\n",
       "      <td>12.992463</td>\n",
       "      <td>14.766232</td>\n",
       "      <td>50.135269</td>\n",
       "      <td>42.389999</td>\n",
       "      <td>30.930000</td>\n",
       "      <td>9.625668</td>\n",
       "      <td>35.099998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>4.89</td>\n",
       "      <td>27.592703</td>\n",
       "      <td>6.776616</td>\n",
       "      <td>12.734218</td>\n",
       "      <td>14.966759</td>\n",
       "      <td>50.465557</td>\n",
       "      <td>41.990002</td>\n",
       "      <td>31.879999</td>\n",
       "      <td>9.764309</td>\n",
       "      <td>35.450001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>4.96</td>\n",
       "      <td>28.535372</td>\n",
       "      <td>6.707061</td>\n",
       "      <td>13.188375</td>\n",
       "      <td>15.249322</td>\n",
       "      <td>51.174698</td>\n",
       "      <td>39.900002</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>9.843532</td>\n",
       "      <td>35.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>4.82</td>\n",
       "      <td>28.407717</td>\n",
       "      <td>6.011514</td>\n",
       "      <td>12.814363</td>\n",
       "      <td>15.121715</td>\n",
       "      <td>50.523842</td>\n",
       "      <td>41.209999</td>\n",
       "      <td>30.559999</td>\n",
       "      <td>9.754405</td>\n",
       "      <td>34.389999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2017-12-06</td>\n",
       "      <td>5.03</td>\n",
       "      <td>28.122952</td>\n",
       "      <td>6.051260</td>\n",
       "      <td>12.324585</td>\n",
       "      <td>14.830035</td>\n",
       "      <td>50.212982</td>\n",
       "      <td>41.580002</td>\n",
       "      <td>28.440001</td>\n",
       "      <td>9.833631</td>\n",
       "      <td>34.889999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2017-12-07</td>\n",
       "      <td>5.18</td>\n",
       "      <td>28.260426</td>\n",
       "      <td>6.091005</td>\n",
       "      <td>12.556117</td>\n",
       "      <td>14.830035</td>\n",
       "      <td>50.854122</td>\n",
       "      <td>43.200001</td>\n",
       "      <td>28.860001</td>\n",
       "      <td>9.556347</td>\n",
       "      <td>35.369999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>5.33</td>\n",
       "      <td>28.525553</td>\n",
       "      <td>6.369224</td>\n",
       "      <td>13.063704</td>\n",
       "      <td>14.647736</td>\n",
       "      <td>51.378693</td>\n",
       "      <td>43.209999</td>\n",
       "      <td>29.389999</td>\n",
       "      <td>9.605862</td>\n",
       "      <td>35.639999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>5.40</td>\n",
       "      <td>28.417538</td>\n",
       "      <td>6.408969</td>\n",
       "      <td>14.132309</td>\n",
       "      <td>14.611277</td>\n",
       "      <td>51.262127</td>\n",
       "      <td>43.009998</td>\n",
       "      <td>30.360001</td>\n",
       "      <td>9.645473</td>\n",
       "      <td>36.439999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2017-12-12</td>\n",
       "      <td>5.33</td>\n",
       "      <td>28.790678</td>\n",
       "      <td>6.627570</td>\n",
       "      <td>13.980924</td>\n",
       "      <td>14.884725</td>\n",
       "      <td>52.311264</td>\n",
       "      <td>41.860001</td>\n",
       "      <td>30.240000</td>\n",
       "      <td>9.843532</td>\n",
       "      <td>36.310001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>5.38</td>\n",
       "      <td>28.319342</td>\n",
       "      <td>6.508334</td>\n",
       "      <td>14.915954</td>\n",
       "      <td>15.039679</td>\n",
       "      <td>51.660408</td>\n",
       "      <td>42.049999</td>\n",
       "      <td>29.070000</td>\n",
       "      <td>9.536541</td>\n",
       "      <td>36.700001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>2017-12-14</td>\n",
       "      <td>5.28</td>\n",
       "      <td>28.211327</td>\n",
       "      <td>6.399033</td>\n",
       "      <td>14.666611</td>\n",
       "      <td>15.221978</td>\n",
       "      <td>51.135838</td>\n",
       "      <td>42.240002</td>\n",
       "      <td>28.629999</td>\n",
       "      <td>9.387997</td>\n",
       "      <td>36.919998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>5.30</td>\n",
       "      <td>28.515734</td>\n",
       "      <td>6.607697</td>\n",
       "      <td>15.129675</td>\n",
       "      <td>15.285784</td>\n",
       "      <td>51.582691</td>\n",
       "      <td>42.400002</td>\n",
       "      <td>27.809999</td>\n",
       "      <td>9.338483</td>\n",
       "      <td>38.130001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>2017-12-18</td>\n",
       "      <td>5.49</td>\n",
       "      <td>28.947788</td>\n",
       "      <td>6.776616</td>\n",
       "      <td>15.806458</td>\n",
       "      <td>15.495424</td>\n",
       "      <td>51.718700</td>\n",
       "      <td>43.709999</td>\n",
       "      <td>28.740000</td>\n",
       "      <td>9.516736</td>\n",
       "      <td>37.970001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>2017-12-19</td>\n",
       "      <td>5.53</td>\n",
       "      <td>28.918329</td>\n",
       "      <td>6.667315</td>\n",
       "      <td>15.494781</td>\n",
       "      <td>15.367818</td>\n",
       "      <td>51.417553</td>\n",
       "      <td>43.980000</td>\n",
       "      <td>29.840000</td>\n",
       "      <td>9.526638</td>\n",
       "      <td>37.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>2017-12-20</td>\n",
       "      <td>5.63</td>\n",
       "      <td>28.947788</td>\n",
       "      <td>6.836234</td>\n",
       "      <td>15.272154</td>\n",
       "      <td>15.331358</td>\n",
       "      <td>51.009552</td>\n",
       "      <td>45.750000</td>\n",
       "      <td>30.430000</td>\n",
       "      <td>9.655376</td>\n",
       "      <td>37.580002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2017-12-21</td>\n",
       "      <td>5.72</td>\n",
       "      <td>29.281649</td>\n",
       "      <td>7.104517</td>\n",
       "      <td>15.557117</td>\n",
       "      <td>15.322242</td>\n",
       "      <td>51.368984</td>\n",
       "      <td>44.419998</td>\n",
       "      <td>31.010000</td>\n",
       "      <td>10.021786</td>\n",
       "      <td>38.220001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2017-12-22</td>\n",
       "      <td>5.71</td>\n",
       "      <td>29.340567</td>\n",
       "      <td>7.233690</td>\n",
       "      <td>15.352301</td>\n",
       "      <td>15.358704</td>\n",
       "      <td>51.213554</td>\n",
       "      <td>44.119999</td>\n",
       "      <td>31.459999</td>\n",
       "      <td>9.893046</td>\n",
       "      <td>38.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>5.78</td>\n",
       "      <td>29.242374</td>\n",
       "      <td>7.263499</td>\n",
       "      <td>15.459160</td>\n",
       "      <td>15.531888</td>\n",
       "      <td>50.970695</td>\n",
       "      <td>42.250000</td>\n",
       "      <td>32.189999</td>\n",
       "      <td>10.061398</td>\n",
       "      <td>38.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>5.67</td>\n",
       "      <td>29.193274</td>\n",
       "      <td>7.164135</td>\n",
       "      <td>15.200914</td>\n",
       "      <td>15.495424</td>\n",
       "      <td>51.067841</td>\n",
       "      <td>42.480000</td>\n",
       "      <td>31.820000</td>\n",
       "      <td>10.001980</td>\n",
       "      <td>38.439999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2017-12-28</td>\n",
       "      <td>5.76</td>\n",
       "      <td>29.262011</td>\n",
       "      <td>7.283371</td>\n",
       "      <td>15.245440</td>\n",
       "      <td>15.668611</td>\n",
       "      <td>51.145554</td>\n",
       "      <td>41.810001</td>\n",
       "      <td>31.980000</td>\n",
       "      <td>10.120814</td>\n",
       "      <td>38.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>5.66</td>\n",
       "      <td>28.987066</td>\n",
       "      <td>7.164135</td>\n",
       "      <td>14.853620</td>\n",
       "      <td>15.732414</td>\n",
       "      <td>50.970695</td>\n",
       "      <td>41.119999</td>\n",
       "      <td>31.530001</td>\n",
       "      <td>10.190136</td>\n",
       "      <td>37.840000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date    AKS        BAC        CLF        CTL         ET         MS  \\\n",
       "0    2017-01-03  10.54  21.779140   8.555229  20.231129  16.380995  41.021313   \n",
       "1    2017-01-04  11.11  22.185143   8.803638  20.334877  16.722797  41.564457   \n",
       "2    2017-01-05  10.88  21.924143   8.664530  20.350840  16.731340  41.183304   \n",
       "3    2017-01-06  10.72  21.924143   8.416120  20.255070  16.970608  41.783615   \n",
       "4    2017-01-09  10.31  21.798471   8.376374  20.135357  16.167364  40.697342   \n",
       "5    2017-01-10  11.05  22.175476   9.330268  20.111420  16.013552  41.030857   \n",
       "6    2017-01-11  11.02  22.301144   9.727723  20.143341  16.090458  41.612099   \n",
       "7    2017-01-12   9.87  22.156145   9.469377  20.095455  16.064823  41.573994   \n",
       "8    2017-01-13   9.71  22.243143   9.171285  20.159302  15.893919  41.745514   \n",
       "9    2017-01-17   9.45  21.315136   9.101730  20.326895  15.842651  40.163727   \n",
       "10   2017-01-18  10.07  21.875805   9.370013  20.326895  15.381215  40.840271   \n",
       "11   2017-01-19   9.52  21.779140   8.952684  20.350840  15.133404  40.449596   \n",
       "12   2017-01-20   9.52  21.885473   8.674465  20.398720  15.398306  40.583000   \n",
       "13   2017-01-23   9.29  21.808140   8.724148  20.598238  15.594842  39.982689   \n",
       "14   2017-01-24   8.68  22.185143   9.230904  20.406704  15.911013  40.716400   \n",
       "15   2017-01-25   8.26  22.591145   8.912939  20.502470  16.167364  41.793148   \n",
       "16   2017-01-26   8.19  22.658810   8.813575  20.797758  16.201542  41.945610   \n",
       "17   2017-01-27   8.07  22.581478   8.962622  20.717951  15.859742  41.782875   \n",
       "18   2017-01-30   8.09  22.185143   8.614847  20.598238  15.398306  41.275551   \n",
       "19   2017-01-31   8.08  21.885473   8.714211  20.638147  15.338489  40.672504   \n",
       "20   2017-02-01   7.79  22.127142   8.724148  20.358822  15.458119  40.873516   \n",
       "21   2017-02-02   7.92  21.962805   9.141476  20.159302  15.817014  40.327904   \n",
       "22   2017-02-03   8.13  22.513815   8.734084  20.286993  16.533009  42.529518   \n",
       "23   2017-02-06   8.20  22.349480   9.042112  19.991703  16.090397  42.720959   \n",
       "24   2017-02-07   8.01  22.136806   9.061985  19.856033  16.533009  42.950695   \n",
       "25   2017-02-08   8.04  21.914476   9.459441  19.496901  16.706587  42.031757   \n",
       "26   2017-02-09   8.66  22.349480  11.297672  19.488920  16.619799  42.931549   \n",
       "27   2017-02-10   8.39  22.310810  11.079071  19.544785  16.350758  42.787968   \n",
       "28   2017-02-13   8.76  22.620148  11.844173  19.648535  16.125111  43.324013   \n",
       "29   2017-02-14   8.57  23.258148  11.953473  19.584692  16.819407  44.070652   \n",
       "..          ...    ...        ...        ...        ...        ...        ...   \n",
       "221  2017-11-16   4.39  26.165396   5.981705  12.640324  15.167289  47.211277   \n",
       "222  2017-11-17   4.51  26.028505   6.091005  12.683171  14.866495  47.308418   \n",
       "223  2017-11-20   4.63  26.145838   6.130751  12.683171  14.748001  47.706711   \n",
       "224  2017-11-21   4.85  26.136061   6.259924  12.451789  14.629505  47.968994   \n",
       "225  2017-11-22   4.88  26.067616   6.339415  12.288965  14.738886  47.687283   \n",
       "226  2017-11-24   4.88  25.999172   6.567952  12.324585  14.793575  47.658138   \n",
       "227  2017-11-27   4.71  25.999172   6.339415  12.128674  14.675081  47.599854   \n",
       "228  2017-11-28   4.79  27.025839   6.577888  12.191010  14.638620  48.959843   \n",
       "229  2017-11-29   4.89  27.651619   6.369224  12.947938  14.392516  49.853554   \n",
       "230  2017-11-30   4.87  27.661440   6.617633  12.992463  14.766232  50.135269   \n",
       "231  2017-12-01   4.89  27.592703   6.776616  12.734218  14.966759  50.465557   \n",
       "232  2017-12-04   4.96  28.535372   6.707061  13.188375  15.249322  51.174698   \n",
       "233  2017-12-05   4.82  28.407717   6.011514  12.814363  15.121715  50.523842   \n",
       "234  2017-12-06   5.03  28.122952   6.051260  12.324585  14.830035  50.212982   \n",
       "235  2017-12-07   5.18  28.260426   6.091005  12.556117  14.830035  50.854122   \n",
       "236  2017-12-08   5.33  28.525553   6.369224  13.063704  14.647736  51.378693   \n",
       "237  2017-12-11   5.40  28.417538   6.408969  14.132309  14.611277  51.262127   \n",
       "238  2017-12-12   5.33  28.790678   6.627570  13.980924  14.884725  52.311264   \n",
       "239  2017-12-13   5.38  28.319342   6.508334  14.915954  15.039679  51.660408   \n",
       "240  2017-12-14   5.28  28.211327   6.399033  14.666611  15.221978  51.135838   \n",
       "241  2017-12-15   5.30  28.515734   6.607697  15.129675  15.285784  51.582691   \n",
       "242  2017-12-18   5.49  28.947788   6.776616  15.806458  15.495424  51.718700   \n",
       "243  2017-12-19   5.53  28.918329   6.667315  15.494781  15.367818  51.417553   \n",
       "244  2017-12-20   5.63  28.947788   6.836234  15.272154  15.331358  51.009552   \n",
       "245  2017-12-21   5.72  29.281649   7.104517  15.557117  15.322242  51.368984   \n",
       "246  2017-12-22   5.71  29.340567   7.233690  15.352301  15.358704  51.213554   \n",
       "247  2017-12-26   5.78  29.242374   7.263499  15.459160  15.531888  50.970695   \n",
       "248  2017-12-27   5.67  29.193274   7.164135  15.200914  15.495424  51.067841   \n",
       "249  2017-12-28   5.76  29.262011   7.283371  15.245440  15.668611  51.145554   \n",
       "250  2017-12-29   5.66  28.987066   7.164135  14.853620  15.732414  50.970695   \n",
       "\n",
       "            MU        NFX        PBR        SKX  \n",
       "0          NaN  42.160000  10.724895  24.889999  \n",
       "1          NaN  42.490002  10.695187  25.610001  \n",
       "2          NaN  42.980000  10.992275  25.219999  \n",
       "3          NaN  42.970001  10.764507  25.030001  \n",
       "4          NaN  42.320000  10.724895  25.150000  \n",
       "5          NaN  41.610001  10.913053  25.620001  \n",
       "6          NaN  42.150002  11.428003  25.240000  \n",
       "7          NaN  42.500000  11.378489  25.510000  \n",
       "8          NaN  41.900002  11.210139  25.459999  \n",
       "9          NaN  42.110001  11.160625  25.270000  \n",
       "10   22.320000  41.669998  11.061595  25.230000  \n",
       "11   21.709999  41.619999  11.031886  24.549999  \n",
       "12   21.959999  41.820000  11.091304  24.969999  \n",
       "13   21.889999  41.160000  11.220043  24.860001  \n",
       "14   22.850000  42.419998  10.992275  25.900000  \n",
       "15         NaN  42.500000  10.863537  25.780001  \n",
       "16   23.520000  42.160000  10.804118  24.980000  \n",
       "17   23.969999  41.779999  10.685283  24.969999  \n",
       "18   24.180000  40.119999  10.289166  24.920000  \n",
       "19   24.110001  40.080002  10.160426  25.120001  \n",
       "20   24.750000  39.889999  10.130718  24.290001  \n",
       "21   24.790001  40.509998  10.051494  23.660000  \n",
       "22   24.600000  41.630001  10.229747  23.510000  \n",
       "23   24.340000  40.709999  10.091105  23.260000  \n",
       "24   24.600000  39.020000   9.982173  22.590000  \n",
       "25   24.209999  39.560001  10.140620  22.540001  \n",
       "26   24.450001  40.910000  10.041593  23.280001  \n",
       "27   24.049999  41.770000  10.318873  27.780001  \n",
       "28   23.900000  42.000000  10.467419  26.990000  \n",
       "29   23.120001  43.669998  10.804118  26.639999  \n",
       "..         ...        ...        ...        ...  \n",
       "221  46.180000  29.440001   9.873241  32.799999  \n",
       "222  46.160000  30.299999  10.041593  33.619999  \n",
       "223  47.639999  30.010000  10.071300  33.709999  \n",
       "224  49.400002  30.180000   9.932658  33.180000  \n",
       "225  49.139999  30.660000  10.160426  33.669998  \n",
       "226  49.680000  30.740000  10.219845  33.540001  \n",
       "227  48.049999  29.690001  10.021786  34.689999  \n",
       "228  47.930000  29.920000  10.061398  34.610001  \n",
       "229  43.740002  30.100000   9.714794  34.560001  \n",
       "230  42.389999  30.930000   9.625668  35.099998  \n",
       "231  41.990002  31.879999   9.764309  35.450001  \n",
       "232  39.900002  31.400000   9.843532  35.180000  \n",
       "233  41.209999  30.559999   9.754405  34.389999  \n",
       "234  41.580002  28.440001   9.833631  34.889999  \n",
       "235  43.200001  28.860001   9.556347  35.369999  \n",
       "236  43.209999  29.389999   9.605862  35.639999  \n",
       "237  43.009998  30.360001   9.645473  36.439999  \n",
       "238  41.860001  30.240000   9.843532  36.310001  \n",
       "239  42.049999  29.070000   9.536541  36.700001  \n",
       "240  42.240002  28.629999   9.387997  36.919998  \n",
       "241  42.400002  27.809999   9.338483  38.130001  \n",
       "242  43.709999  28.740000   9.516736  37.970001  \n",
       "243  43.980000  29.840000   9.526638  37.610001  \n",
       "244  45.750000  30.430000   9.655376  37.580002  \n",
       "245  44.419998  31.010000  10.021786  38.220001  \n",
       "246  44.119999  31.459999   9.893046  38.320000  \n",
       "247  42.250000  32.189999  10.061398  38.660000  \n",
       "248  42.480000  31.820000  10.001980  38.439999  \n",
       "249  41.810001  31.980000  10.120814  38.520000  \n",
       "250  41.119999  31.530001  10.190136  37.840000  \n",
       "\n",
       "[251 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['MU'][:10] = None\n",
    "\n",
    "data['MU'][15] = None\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AKS</th>\n",
       "      <th>BAC</th>\n",
       "      <th>CLF</th>\n",
       "      <th>CTL</th>\n",
       "      <th>ET</th>\n",
       "      <th>MS</th>\n",
       "      <th>MU</th>\n",
       "      <th>NFX</th>\n",
       "      <th>PBR</th>\n",
       "      <th>SKX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>10.54</td>\n",
       "      <td>21.779140</td>\n",
       "      <td>8.555229</td>\n",
       "      <td>20.231129</td>\n",
       "      <td>16.380995</td>\n",
       "      <td>41.021313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.160000</td>\n",
       "      <td>10.724895</td>\n",
       "      <td>24.889999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>11.11</td>\n",
       "      <td>22.185143</td>\n",
       "      <td>8.803638</td>\n",
       "      <td>20.334877</td>\n",
       "      <td>16.722797</td>\n",
       "      <td>41.564457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.490002</td>\n",
       "      <td>10.695187</td>\n",
       "      <td>25.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>10.88</td>\n",
       "      <td>21.924143</td>\n",
       "      <td>8.664530</td>\n",
       "      <td>20.350840</td>\n",
       "      <td>16.731340</td>\n",
       "      <td>41.183304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.980000</td>\n",
       "      <td>10.992275</td>\n",
       "      <td>25.219999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>10.72</td>\n",
       "      <td>21.924143</td>\n",
       "      <td>8.416120</td>\n",
       "      <td>20.255070</td>\n",
       "      <td>16.970608</td>\n",
       "      <td>41.783615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.970001</td>\n",
       "      <td>10.764507</td>\n",
       "      <td>25.030001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>10.31</td>\n",
       "      <td>21.798471</td>\n",
       "      <td>8.376374</td>\n",
       "      <td>20.135357</td>\n",
       "      <td>16.167364</td>\n",
       "      <td>40.697342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.320000</td>\n",
       "      <td>10.724895</td>\n",
       "      <td>25.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>11.05</td>\n",
       "      <td>22.175476</td>\n",
       "      <td>9.330268</td>\n",
       "      <td>20.111420</td>\n",
       "      <td>16.013552</td>\n",
       "      <td>41.030857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.610001</td>\n",
       "      <td>10.913053</td>\n",
       "      <td>25.620001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>11.02</td>\n",
       "      <td>22.301144</td>\n",
       "      <td>9.727723</td>\n",
       "      <td>20.143341</td>\n",
       "      <td>16.090458</td>\n",
       "      <td>41.612099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.150002</td>\n",
       "      <td>11.428003</td>\n",
       "      <td>25.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>9.87</td>\n",
       "      <td>22.156145</td>\n",
       "      <td>9.469377</td>\n",
       "      <td>20.095455</td>\n",
       "      <td>16.064823</td>\n",
       "      <td>41.573994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>11.378489</td>\n",
       "      <td>25.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>9.71</td>\n",
       "      <td>22.243143</td>\n",
       "      <td>9.171285</td>\n",
       "      <td>20.159302</td>\n",
       "      <td>15.893919</td>\n",
       "      <td>41.745514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.900002</td>\n",
       "      <td>11.210139</td>\n",
       "      <td>25.459999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>9.45</td>\n",
       "      <td>21.315136</td>\n",
       "      <td>9.101730</td>\n",
       "      <td>20.326895</td>\n",
       "      <td>15.842651</td>\n",
       "      <td>40.163727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.110001</td>\n",
       "      <td>11.160625</td>\n",
       "      <td>25.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-01-18</td>\n",
       "      <td>10.07</td>\n",
       "      <td>21.875805</td>\n",
       "      <td>9.370013</td>\n",
       "      <td>20.326895</td>\n",
       "      <td>15.381215</td>\n",
       "      <td>40.840271</td>\n",
       "      <td>22.320000</td>\n",
       "      <td>41.669998</td>\n",
       "      <td>11.061595</td>\n",
       "      <td>25.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017-01-19</td>\n",
       "      <td>9.52</td>\n",
       "      <td>21.779140</td>\n",
       "      <td>8.952684</td>\n",
       "      <td>20.350840</td>\n",
       "      <td>15.133404</td>\n",
       "      <td>40.449596</td>\n",
       "      <td>21.709999</td>\n",
       "      <td>41.619999</td>\n",
       "      <td>11.031886</td>\n",
       "      <td>24.549999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-01-20</td>\n",
       "      <td>9.52</td>\n",
       "      <td>21.885473</td>\n",
       "      <td>8.674465</td>\n",
       "      <td>20.398720</td>\n",
       "      <td>15.398306</td>\n",
       "      <td>40.583000</td>\n",
       "      <td>21.959999</td>\n",
       "      <td>41.820000</td>\n",
       "      <td>11.091304</td>\n",
       "      <td>24.969999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>9.29</td>\n",
       "      <td>21.808140</td>\n",
       "      <td>8.724148</td>\n",
       "      <td>20.598238</td>\n",
       "      <td>15.594842</td>\n",
       "      <td>39.982689</td>\n",
       "      <td>21.889999</td>\n",
       "      <td>41.160000</td>\n",
       "      <td>11.220043</td>\n",
       "      <td>24.860001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-01-24</td>\n",
       "      <td>8.68</td>\n",
       "      <td>22.185143</td>\n",
       "      <td>9.230904</td>\n",
       "      <td>20.406704</td>\n",
       "      <td>15.911013</td>\n",
       "      <td>40.716400</td>\n",
       "      <td>22.850000</td>\n",
       "      <td>42.419998</td>\n",
       "      <td>10.992275</td>\n",
       "      <td>25.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-01-25</td>\n",
       "      <td>8.26</td>\n",
       "      <td>22.591145</td>\n",
       "      <td>8.912939</td>\n",
       "      <td>20.502470</td>\n",
       "      <td>16.167364</td>\n",
       "      <td>41.793148</td>\n",
       "      <td>23.185000</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>10.863537</td>\n",
       "      <td>25.780001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017-01-26</td>\n",
       "      <td>8.19</td>\n",
       "      <td>22.658810</td>\n",
       "      <td>8.813575</td>\n",
       "      <td>20.797758</td>\n",
       "      <td>16.201542</td>\n",
       "      <td>41.945610</td>\n",
       "      <td>23.520000</td>\n",
       "      <td>42.160000</td>\n",
       "      <td>10.804118</td>\n",
       "      <td>24.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>8.07</td>\n",
       "      <td>22.581478</td>\n",
       "      <td>8.962622</td>\n",
       "      <td>20.717951</td>\n",
       "      <td>15.859742</td>\n",
       "      <td>41.782875</td>\n",
       "      <td>23.969999</td>\n",
       "      <td>41.779999</td>\n",
       "      <td>10.685283</td>\n",
       "      <td>24.969999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017-01-30</td>\n",
       "      <td>8.09</td>\n",
       "      <td>22.185143</td>\n",
       "      <td>8.614847</td>\n",
       "      <td>20.598238</td>\n",
       "      <td>15.398306</td>\n",
       "      <td>41.275551</td>\n",
       "      <td>24.180000</td>\n",
       "      <td>40.119999</td>\n",
       "      <td>10.289166</td>\n",
       "      <td>24.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>8.08</td>\n",
       "      <td>21.885473</td>\n",
       "      <td>8.714211</td>\n",
       "      <td>20.638147</td>\n",
       "      <td>15.338489</td>\n",
       "      <td>40.672504</td>\n",
       "      <td>24.110001</td>\n",
       "      <td>40.080002</td>\n",
       "      <td>10.160426</td>\n",
       "      <td>25.120001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>7.79</td>\n",
       "      <td>22.127142</td>\n",
       "      <td>8.724148</td>\n",
       "      <td>20.358822</td>\n",
       "      <td>15.458119</td>\n",
       "      <td>40.873516</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>39.889999</td>\n",
       "      <td>10.130718</td>\n",
       "      <td>24.290001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017-02-02</td>\n",
       "      <td>7.92</td>\n",
       "      <td>21.962805</td>\n",
       "      <td>9.141476</td>\n",
       "      <td>20.159302</td>\n",
       "      <td>15.817014</td>\n",
       "      <td>40.327904</td>\n",
       "      <td>24.790001</td>\n",
       "      <td>40.509998</td>\n",
       "      <td>10.051494</td>\n",
       "      <td>23.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017-02-03</td>\n",
       "      <td>8.13</td>\n",
       "      <td>22.513815</td>\n",
       "      <td>8.734084</td>\n",
       "      <td>20.286993</td>\n",
       "      <td>16.533009</td>\n",
       "      <td>42.529518</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>41.630001</td>\n",
       "      <td>10.229747</td>\n",
       "      <td>23.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>8.20</td>\n",
       "      <td>22.349480</td>\n",
       "      <td>9.042112</td>\n",
       "      <td>19.991703</td>\n",
       "      <td>16.090397</td>\n",
       "      <td>42.720959</td>\n",
       "      <td>24.340000</td>\n",
       "      <td>40.709999</td>\n",
       "      <td>10.091105</td>\n",
       "      <td>23.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2017-02-07</td>\n",
       "      <td>8.01</td>\n",
       "      <td>22.136806</td>\n",
       "      <td>9.061985</td>\n",
       "      <td>19.856033</td>\n",
       "      <td>16.533009</td>\n",
       "      <td>42.950695</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>39.020000</td>\n",
       "      <td>9.982173</td>\n",
       "      <td>22.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2017-02-08</td>\n",
       "      <td>8.04</td>\n",
       "      <td>21.914476</td>\n",
       "      <td>9.459441</td>\n",
       "      <td>19.496901</td>\n",
       "      <td>16.706587</td>\n",
       "      <td>42.031757</td>\n",
       "      <td>24.209999</td>\n",
       "      <td>39.560001</td>\n",
       "      <td>10.140620</td>\n",
       "      <td>22.540001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017-02-09</td>\n",
       "      <td>8.66</td>\n",
       "      <td>22.349480</td>\n",
       "      <td>11.297672</td>\n",
       "      <td>19.488920</td>\n",
       "      <td>16.619799</td>\n",
       "      <td>42.931549</td>\n",
       "      <td>24.450001</td>\n",
       "      <td>40.910000</td>\n",
       "      <td>10.041593</td>\n",
       "      <td>23.280001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017-02-10</td>\n",
       "      <td>8.39</td>\n",
       "      <td>22.310810</td>\n",
       "      <td>11.079071</td>\n",
       "      <td>19.544785</td>\n",
       "      <td>16.350758</td>\n",
       "      <td>42.787968</td>\n",
       "      <td>24.049999</td>\n",
       "      <td>41.770000</td>\n",
       "      <td>10.318873</td>\n",
       "      <td>27.780001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017-02-13</td>\n",
       "      <td>8.76</td>\n",
       "      <td>22.620148</td>\n",
       "      <td>11.844173</td>\n",
       "      <td>19.648535</td>\n",
       "      <td>16.125111</td>\n",
       "      <td>43.324013</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>10.467419</td>\n",
       "      <td>26.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017-02-14</td>\n",
       "      <td>8.57</td>\n",
       "      <td>23.258148</td>\n",
       "      <td>11.953473</td>\n",
       "      <td>19.584692</td>\n",
       "      <td>16.819407</td>\n",
       "      <td>44.070652</td>\n",
       "      <td>23.120001</td>\n",
       "      <td>43.669998</td>\n",
       "      <td>10.804118</td>\n",
       "      <td>26.639999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>4.39</td>\n",
       "      <td>26.165396</td>\n",
       "      <td>5.981705</td>\n",
       "      <td>12.640324</td>\n",
       "      <td>15.167289</td>\n",
       "      <td>47.211277</td>\n",
       "      <td>46.180000</td>\n",
       "      <td>29.440001</td>\n",
       "      <td>9.873241</td>\n",
       "      <td>32.799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>2017-11-17</td>\n",
       "      <td>4.51</td>\n",
       "      <td>26.028505</td>\n",
       "      <td>6.091005</td>\n",
       "      <td>12.683171</td>\n",
       "      <td>14.866495</td>\n",
       "      <td>47.308418</td>\n",
       "      <td>46.160000</td>\n",
       "      <td>30.299999</td>\n",
       "      <td>10.041593</td>\n",
       "      <td>33.619999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2017-11-20</td>\n",
       "      <td>4.63</td>\n",
       "      <td>26.145838</td>\n",
       "      <td>6.130751</td>\n",
       "      <td>12.683171</td>\n",
       "      <td>14.748001</td>\n",
       "      <td>47.706711</td>\n",
       "      <td>47.639999</td>\n",
       "      <td>30.010000</td>\n",
       "      <td>10.071300</td>\n",
       "      <td>33.709999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>4.85</td>\n",
       "      <td>26.136061</td>\n",
       "      <td>6.259924</td>\n",
       "      <td>12.451789</td>\n",
       "      <td>14.629505</td>\n",
       "      <td>47.968994</td>\n",
       "      <td>49.400002</td>\n",
       "      <td>30.180000</td>\n",
       "      <td>9.932658</td>\n",
       "      <td>33.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2017-11-22</td>\n",
       "      <td>4.88</td>\n",
       "      <td>26.067616</td>\n",
       "      <td>6.339415</td>\n",
       "      <td>12.288965</td>\n",
       "      <td>14.738886</td>\n",
       "      <td>47.687283</td>\n",
       "      <td>49.139999</td>\n",
       "      <td>30.660000</td>\n",
       "      <td>10.160426</td>\n",
       "      <td>33.669998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2017-11-24</td>\n",
       "      <td>4.88</td>\n",
       "      <td>25.999172</td>\n",
       "      <td>6.567952</td>\n",
       "      <td>12.324585</td>\n",
       "      <td>14.793575</td>\n",
       "      <td>47.658138</td>\n",
       "      <td>49.680000</td>\n",
       "      <td>30.740000</td>\n",
       "      <td>10.219845</td>\n",
       "      <td>33.540001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>4.71</td>\n",
       "      <td>25.999172</td>\n",
       "      <td>6.339415</td>\n",
       "      <td>12.128674</td>\n",
       "      <td>14.675081</td>\n",
       "      <td>47.599854</td>\n",
       "      <td>48.049999</td>\n",
       "      <td>29.690001</td>\n",
       "      <td>10.021786</td>\n",
       "      <td>34.689999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>4.79</td>\n",
       "      <td>27.025839</td>\n",
       "      <td>6.577888</td>\n",
       "      <td>12.191010</td>\n",
       "      <td>14.638620</td>\n",
       "      <td>48.959843</td>\n",
       "      <td>47.930000</td>\n",
       "      <td>29.920000</td>\n",
       "      <td>10.061398</td>\n",
       "      <td>34.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>4.89</td>\n",
       "      <td>27.651619</td>\n",
       "      <td>6.369224</td>\n",
       "      <td>12.947938</td>\n",
       "      <td>14.392516</td>\n",
       "      <td>49.853554</td>\n",
       "      <td>43.740002</td>\n",
       "      <td>30.100000</td>\n",
       "      <td>9.714794</td>\n",
       "      <td>34.560001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>4.87</td>\n",
       "      <td>27.661440</td>\n",
       "      <td>6.617633</td>\n",
       "      <td>12.992463</td>\n",
       "      <td>14.766232</td>\n",
       "      <td>50.135269</td>\n",
       "      <td>42.389999</td>\n",
       "      <td>30.930000</td>\n",
       "      <td>9.625668</td>\n",
       "      <td>35.099998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>4.89</td>\n",
       "      <td>27.592703</td>\n",
       "      <td>6.776616</td>\n",
       "      <td>12.734218</td>\n",
       "      <td>14.966759</td>\n",
       "      <td>50.465557</td>\n",
       "      <td>41.990002</td>\n",
       "      <td>31.879999</td>\n",
       "      <td>9.764309</td>\n",
       "      <td>35.450001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>4.96</td>\n",
       "      <td>28.535372</td>\n",
       "      <td>6.707061</td>\n",
       "      <td>13.188375</td>\n",
       "      <td>15.249322</td>\n",
       "      <td>51.174698</td>\n",
       "      <td>39.900002</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>9.843532</td>\n",
       "      <td>35.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>4.82</td>\n",
       "      <td>28.407717</td>\n",
       "      <td>6.011514</td>\n",
       "      <td>12.814363</td>\n",
       "      <td>15.121715</td>\n",
       "      <td>50.523842</td>\n",
       "      <td>41.209999</td>\n",
       "      <td>30.559999</td>\n",
       "      <td>9.754405</td>\n",
       "      <td>34.389999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2017-12-06</td>\n",
       "      <td>5.03</td>\n",
       "      <td>28.122952</td>\n",
       "      <td>6.051260</td>\n",
       "      <td>12.324585</td>\n",
       "      <td>14.830035</td>\n",
       "      <td>50.212982</td>\n",
       "      <td>41.580002</td>\n",
       "      <td>28.440001</td>\n",
       "      <td>9.833631</td>\n",
       "      <td>34.889999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2017-12-07</td>\n",
       "      <td>5.18</td>\n",
       "      <td>28.260426</td>\n",
       "      <td>6.091005</td>\n",
       "      <td>12.556117</td>\n",
       "      <td>14.830035</td>\n",
       "      <td>50.854122</td>\n",
       "      <td>43.200001</td>\n",
       "      <td>28.860001</td>\n",
       "      <td>9.556347</td>\n",
       "      <td>35.369999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>5.33</td>\n",
       "      <td>28.525553</td>\n",
       "      <td>6.369224</td>\n",
       "      <td>13.063704</td>\n",
       "      <td>14.647736</td>\n",
       "      <td>51.378693</td>\n",
       "      <td>43.209999</td>\n",
       "      <td>29.389999</td>\n",
       "      <td>9.605862</td>\n",
       "      <td>35.639999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>5.40</td>\n",
       "      <td>28.417538</td>\n",
       "      <td>6.408969</td>\n",
       "      <td>14.132309</td>\n",
       "      <td>14.611277</td>\n",
       "      <td>51.262127</td>\n",
       "      <td>43.009998</td>\n",
       "      <td>30.360001</td>\n",
       "      <td>9.645473</td>\n",
       "      <td>36.439999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2017-12-12</td>\n",
       "      <td>5.33</td>\n",
       "      <td>28.790678</td>\n",
       "      <td>6.627570</td>\n",
       "      <td>13.980924</td>\n",
       "      <td>14.884725</td>\n",
       "      <td>52.311264</td>\n",
       "      <td>41.860001</td>\n",
       "      <td>30.240000</td>\n",
       "      <td>9.843532</td>\n",
       "      <td>36.310001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>5.38</td>\n",
       "      <td>28.319342</td>\n",
       "      <td>6.508334</td>\n",
       "      <td>14.915954</td>\n",
       "      <td>15.039679</td>\n",
       "      <td>51.660408</td>\n",
       "      <td>42.049999</td>\n",
       "      <td>29.070000</td>\n",
       "      <td>9.536541</td>\n",
       "      <td>36.700001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>2017-12-14</td>\n",
       "      <td>5.28</td>\n",
       "      <td>28.211327</td>\n",
       "      <td>6.399033</td>\n",
       "      <td>14.666611</td>\n",
       "      <td>15.221978</td>\n",
       "      <td>51.135838</td>\n",
       "      <td>42.240002</td>\n",
       "      <td>28.629999</td>\n",
       "      <td>9.387997</td>\n",
       "      <td>36.919998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>5.30</td>\n",
       "      <td>28.515734</td>\n",
       "      <td>6.607697</td>\n",
       "      <td>15.129675</td>\n",
       "      <td>15.285784</td>\n",
       "      <td>51.582691</td>\n",
       "      <td>42.400002</td>\n",
       "      <td>27.809999</td>\n",
       "      <td>9.338483</td>\n",
       "      <td>38.130001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>2017-12-18</td>\n",
       "      <td>5.49</td>\n",
       "      <td>28.947788</td>\n",
       "      <td>6.776616</td>\n",
       "      <td>15.806458</td>\n",
       "      <td>15.495424</td>\n",
       "      <td>51.718700</td>\n",
       "      <td>43.709999</td>\n",
       "      <td>28.740000</td>\n",
       "      <td>9.516736</td>\n",
       "      <td>37.970001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>2017-12-19</td>\n",
       "      <td>5.53</td>\n",
       "      <td>28.918329</td>\n",
       "      <td>6.667315</td>\n",
       "      <td>15.494781</td>\n",
       "      <td>15.367818</td>\n",
       "      <td>51.417553</td>\n",
       "      <td>43.980000</td>\n",
       "      <td>29.840000</td>\n",
       "      <td>9.526638</td>\n",
       "      <td>37.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>2017-12-20</td>\n",
       "      <td>5.63</td>\n",
       "      <td>28.947788</td>\n",
       "      <td>6.836234</td>\n",
       "      <td>15.272154</td>\n",
       "      <td>15.331358</td>\n",
       "      <td>51.009552</td>\n",
       "      <td>45.750000</td>\n",
       "      <td>30.430000</td>\n",
       "      <td>9.655376</td>\n",
       "      <td>37.580002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2017-12-21</td>\n",
       "      <td>5.72</td>\n",
       "      <td>29.281649</td>\n",
       "      <td>7.104517</td>\n",
       "      <td>15.557117</td>\n",
       "      <td>15.322242</td>\n",
       "      <td>51.368984</td>\n",
       "      <td>44.419998</td>\n",
       "      <td>31.010000</td>\n",
       "      <td>10.021786</td>\n",
       "      <td>38.220001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2017-12-22</td>\n",
       "      <td>5.71</td>\n",
       "      <td>29.340567</td>\n",
       "      <td>7.233690</td>\n",
       "      <td>15.352301</td>\n",
       "      <td>15.358704</td>\n",
       "      <td>51.213554</td>\n",
       "      <td>44.119999</td>\n",
       "      <td>31.459999</td>\n",
       "      <td>9.893046</td>\n",
       "      <td>38.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>5.78</td>\n",
       "      <td>29.242374</td>\n",
       "      <td>7.263499</td>\n",
       "      <td>15.459160</td>\n",
       "      <td>15.531888</td>\n",
       "      <td>50.970695</td>\n",
       "      <td>42.250000</td>\n",
       "      <td>32.189999</td>\n",
       "      <td>10.061398</td>\n",
       "      <td>38.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>5.67</td>\n",
       "      <td>29.193274</td>\n",
       "      <td>7.164135</td>\n",
       "      <td>15.200914</td>\n",
       "      <td>15.495424</td>\n",
       "      <td>51.067841</td>\n",
       "      <td>42.480000</td>\n",
       "      <td>31.820000</td>\n",
       "      <td>10.001980</td>\n",
       "      <td>38.439999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2017-12-28</td>\n",
       "      <td>5.76</td>\n",
       "      <td>29.262011</td>\n",
       "      <td>7.283371</td>\n",
       "      <td>15.245440</td>\n",
       "      <td>15.668611</td>\n",
       "      <td>51.145554</td>\n",
       "      <td>41.810001</td>\n",
       "      <td>31.980000</td>\n",
       "      <td>10.120814</td>\n",
       "      <td>38.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>5.66</td>\n",
       "      <td>28.987066</td>\n",
       "      <td>7.164135</td>\n",
       "      <td>14.853620</td>\n",
       "      <td>15.732414</td>\n",
       "      <td>50.970695</td>\n",
       "      <td>41.119999</td>\n",
       "      <td>31.530001</td>\n",
       "      <td>10.190136</td>\n",
       "      <td>37.840000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date    AKS        BAC        CLF        CTL         ET         MS  \\\n",
       "0    2017-01-03  10.54  21.779140   8.555229  20.231129  16.380995  41.021313   \n",
       "1    2017-01-04  11.11  22.185143   8.803638  20.334877  16.722797  41.564457   \n",
       "2    2017-01-05  10.88  21.924143   8.664530  20.350840  16.731340  41.183304   \n",
       "3    2017-01-06  10.72  21.924143   8.416120  20.255070  16.970608  41.783615   \n",
       "4    2017-01-09  10.31  21.798471   8.376374  20.135357  16.167364  40.697342   \n",
       "5    2017-01-10  11.05  22.175476   9.330268  20.111420  16.013552  41.030857   \n",
       "6    2017-01-11  11.02  22.301144   9.727723  20.143341  16.090458  41.612099   \n",
       "7    2017-01-12   9.87  22.156145   9.469377  20.095455  16.064823  41.573994   \n",
       "8    2017-01-13   9.71  22.243143   9.171285  20.159302  15.893919  41.745514   \n",
       "9    2017-01-17   9.45  21.315136   9.101730  20.326895  15.842651  40.163727   \n",
       "10   2017-01-18  10.07  21.875805   9.370013  20.326895  15.381215  40.840271   \n",
       "11   2017-01-19   9.52  21.779140   8.952684  20.350840  15.133404  40.449596   \n",
       "12   2017-01-20   9.52  21.885473   8.674465  20.398720  15.398306  40.583000   \n",
       "13   2017-01-23   9.29  21.808140   8.724148  20.598238  15.594842  39.982689   \n",
       "14   2017-01-24   8.68  22.185143   9.230904  20.406704  15.911013  40.716400   \n",
       "15   2017-01-25   8.26  22.591145   8.912939  20.502470  16.167364  41.793148   \n",
       "16   2017-01-26   8.19  22.658810   8.813575  20.797758  16.201542  41.945610   \n",
       "17   2017-01-27   8.07  22.581478   8.962622  20.717951  15.859742  41.782875   \n",
       "18   2017-01-30   8.09  22.185143   8.614847  20.598238  15.398306  41.275551   \n",
       "19   2017-01-31   8.08  21.885473   8.714211  20.638147  15.338489  40.672504   \n",
       "20   2017-02-01   7.79  22.127142   8.724148  20.358822  15.458119  40.873516   \n",
       "21   2017-02-02   7.92  21.962805   9.141476  20.159302  15.817014  40.327904   \n",
       "22   2017-02-03   8.13  22.513815   8.734084  20.286993  16.533009  42.529518   \n",
       "23   2017-02-06   8.20  22.349480   9.042112  19.991703  16.090397  42.720959   \n",
       "24   2017-02-07   8.01  22.136806   9.061985  19.856033  16.533009  42.950695   \n",
       "25   2017-02-08   8.04  21.914476   9.459441  19.496901  16.706587  42.031757   \n",
       "26   2017-02-09   8.66  22.349480  11.297672  19.488920  16.619799  42.931549   \n",
       "27   2017-02-10   8.39  22.310810  11.079071  19.544785  16.350758  42.787968   \n",
       "28   2017-02-13   8.76  22.620148  11.844173  19.648535  16.125111  43.324013   \n",
       "29   2017-02-14   8.57  23.258148  11.953473  19.584692  16.819407  44.070652   \n",
       "..          ...    ...        ...        ...        ...        ...        ...   \n",
       "221  2017-11-16   4.39  26.165396   5.981705  12.640324  15.167289  47.211277   \n",
       "222  2017-11-17   4.51  26.028505   6.091005  12.683171  14.866495  47.308418   \n",
       "223  2017-11-20   4.63  26.145838   6.130751  12.683171  14.748001  47.706711   \n",
       "224  2017-11-21   4.85  26.136061   6.259924  12.451789  14.629505  47.968994   \n",
       "225  2017-11-22   4.88  26.067616   6.339415  12.288965  14.738886  47.687283   \n",
       "226  2017-11-24   4.88  25.999172   6.567952  12.324585  14.793575  47.658138   \n",
       "227  2017-11-27   4.71  25.999172   6.339415  12.128674  14.675081  47.599854   \n",
       "228  2017-11-28   4.79  27.025839   6.577888  12.191010  14.638620  48.959843   \n",
       "229  2017-11-29   4.89  27.651619   6.369224  12.947938  14.392516  49.853554   \n",
       "230  2017-11-30   4.87  27.661440   6.617633  12.992463  14.766232  50.135269   \n",
       "231  2017-12-01   4.89  27.592703   6.776616  12.734218  14.966759  50.465557   \n",
       "232  2017-12-04   4.96  28.535372   6.707061  13.188375  15.249322  51.174698   \n",
       "233  2017-12-05   4.82  28.407717   6.011514  12.814363  15.121715  50.523842   \n",
       "234  2017-12-06   5.03  28.122952   6.051260  12.324585  14.830035  50.212982   \n",
       "235  2017-12-07   5.18  28.260426   6.091005  12.556117  14.830035  50.854122   \n",
       "236  2017-12-08   5.33  28.525553   6.369224  13.063704  14.647736  51.378693   \n",
       "237  2017-12-11   5.40  28.417538   6.408969  14.132309  14.611277  51.262127   \n",
       "238  2017-12-12   5.33  28.790678   6.627570  13.980924  14.884725  52.311264   \n",
       "239  2017-12-13   5.38  28.319342   6.508334  14.915954  15.039679  51.660408   \n",
       "240  2017-12-14   5.28  28.211327   6.399033  14.666611  15.221978  51.135838   \n",
       "241  2017-12-15   5.30  28.515734   6.607697  15.129675  15.285784  51.582691   \n",
       "242  2017-12-18   5.49  28.947788   6.776616  15.806458  15.495424  51.718700   \n",
       "243  2017-12-19   5.53  28.918329   6.667315  15.494781  15.367818  51.417553   \n",
       "244  2017-12-20   5.63  28.947788   6.836234  15.272154  15.331358  51.009552   \n",
       "245  2017-12-21   5.72  29.281649   7.104517  15.557117  15.322242  51.368984   \n",
       "246  2017-12-22   5.71  29.340567   7.233690  15.352301  15.358704  51.213554   \n",
       "247  2017-12-26   5.78  29.242374   7.263499  15.459160  15.531888  50.970695   \n",
       "248  2017-12-27   5.67  29.193274   7.164135  15.200914  15.495424  51.067841   \n",
       "249  2017-12-28   5.76  29.262011   7.283371  15.245440  15.668611  51.145554   \n",
       "250  2017-12-29   5.66  28.987066   7.164135  14.853620  15.732414  50.970695   \n",
       "\n",
       "            MU        NFX        PBR        SKX  \n",
       "0     0.000000  42.160000  10.724895  24.889999  \n",
       "1     0.000000  42.490002  10.695187  25.610001  \n",
       "2     0.000000  42.980000  10.992275  25.219999  \n",
       "3     0.000000  42.970001  10.764507  25.030001  \n",
       "4     0.000000  42.320000  10.724895  25.150000  \n",
       "5     0.000000  41.610001  10.913053  25.620001  \n",
       "6     0.000000  42.150002  11.428003  25.240000  \n",
       "7     0.000000  42.500000  11.378489  25.510000  \n",
       "8     0.000000  41.900002  11.210139  25.459999  \n",
       "9     0.000000  42.110001  11.160625  25.270000  \n",
       "10   22.320000  41.669998  11.061595  25.230000  \n",
       "11   21.709999  41.619999  11.031886  24.549999  \n",
       "12   21.959999  41.820000  11.091304  24.969999  \n",
       "13   21.889999  41.160000  11.220043  24.860001  \n",
       "14   22.850000  42.419998  10.992275  25.900000  \n",
       "15   23.185000  42.500000  10.863537  25.780001  \n",
       "16   23.520000  42.160000  10.804118  24.980000  \n",
       "17   23.969999  41.779999  10.685283  24.969999  \n",
       "18   24.180000  40.119999  10.289166  24.920000  \n",
       "19   24.110001  40.080002  10.160426  25.120001  \n",
       "20   24.750000  39.889999  10.130718  24.290001  \n",
       "21   24.790001  40.509998  10.051494  23.660000  \n",
       "22   24.600000  41.630001  10.229747  23.510000  \n",
       "23   24.340000  40.709999  10.091105  23.260000  \n",
       "24   24.600000  39.020000   9.982173  22.590000  \n",
       "25   24.209999  39.560001  10.140620  22.540001  \n",
       "26   24.450001  40.910000  10.041593  23.280001  \n",
       "27   24.049999  41.770000  10.318873  27.780001  \n",
       "28   23.900000  42.000000  10.467419  26.990000  \n",
       "29   23.120001  43.669998  10.804118  26.639999  \n",
       "..         ...        ...        ...        ...  \n",
       "221  46.180000  29.440001   9.873241  32.799999  \n",
       "222  46.160000  30.299999  10.041593  33.619999  \n",
       "223  47.639999  30.010000  10.071300  33.709999  \n",
       "224  49.400002  30.180000   9.932658  33.180000  \n",
       "225  49.139999  30.660000  10.160426  33.669998  \n",
       "226  49.680000  30.740000  10.219845  33.540001  \n",
       "227  48.049999  29.690001  10.021786  34.689999  \n",
       "228  47.930000  29.920000  10.061398  34.610001  \n",
       "229  43.740002  30.100000   9.714794  34.560001  \n",
       "230  42.389999  30.930000   9.625668  35.099998  \n",
       "231  41.990002  31.879999   9.764309  35.450001  \n",
       "232  39.900002  31.400000   9.843532  35.180000  \n",
       "233  41.209999  30.559999   9.754405  34.389999  \n",
       "234  41.580002  28.440001   9.833631  34.889999  \n",
       "235  43.200001  28.860001   9.556347  35.369999  \n",
       "236  43.209999  29.389999   9.605862  35.639999  \n",
       "237  43.009998  30.360001   9.645473  36.439999  \n",
       "238  41.860001  30.240000   9.843532  36.310001  \n",
       "239  42.049999  29.070000   9.536541  36.700001  \n",
       "240  42.240002  28.629999   9.387997  36.919998  \n",
       "241  42.400002  27.809999   9.338483  38.130001  \n",
       "242  43.709999  28.740000   9.516736  37.970001  \n",
       "243  43.980000  29.840000   9.526638  37.610001  \n",
       "244  45.750000  30.430000   9.655376  37.580002  \n",
       "245  44.419998  31.010000  10.021786  38.220001  \n",
       "246  44.119999  31.459999   9.893046  38.320000  \n",
       "247  42.250000  32.189999  10.061398  38.660000  \n",
       "248  42.480000  31.820000  10.001980  38.439999  \n",
       "249  41.810001  31.980000  10.120814  38.520000  \n",
       "250  41.119999  31.530001  10.190136  37.840000  \n",
       "\n",
       "[251 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.interpolate(limit_direction='both', limit_area='inside')\n",
    "\n",
    "data = data.fillna(0)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns:\n",
    "    scaler = []\n",
    "    if c != 'Date':\n",
    "        sc = MinMaxScaler()\n",
    "        scaler.append(sc.fit(data[[c]]))\n",
    "        data[c] = sc.transform(data[[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformData(data, pastlag, futurelag = 1, validation_span = 16, arraylength = None):\n",
    "\n",
    "    if arraylength is None:\n",
    "        arraylength = pastlag\n",
    "    cols = []\n",
    "    past_train = []\n",
    "    future_train  = []\n",
    "    past_validate = []\n",
    "    future_validate  = []\n",
    "    for c in data.columns:\n",
    "        if c != 'Date':\n",
    "            cols.append(c)\n",
    "\n",
    "    for c in cols:    \n",
    "        ar = np.asarray(data[c])\n",
    "        l = len(ar)\n",
    "        iv = (l-futurelag) - validation_span\n",
    "        #iv = pastlag + int(((l-futurelag)-pastlag)*(1-validation_percentage))\n",
    "        for i in range(pastlag,len(ar)-futurelag + 1):\n",
    "            if i <= iv:\n",
    "                for j in range(futurelag):\n",
    "                    if i-arraylength < 0:\n",
    "                        p_ar = ar[0:i]\n",
    "                        p_ar = np.pad(p_ar,(arraylength - len(p_ar),0),'constant')\n",
    "                    else:\n",
    "                        p_ar = ar[i-arraylength:i]\n",
    "                    past_train.append(p_ar)\n",
    "                    #future_train.append(ar[i:i+futurelag])\n",
    "                    future_train.append(ar[i+j:i+j+1])\n",
    "            else:\n",
    "                for j in range(futurelag):\n",
    "                    if i-arraylength < 0:\n",
    "                        p_ar = ar[0:i]\n",
    "                        p_ar = np.pad(p_ar,(arraylength - len(p_ar),0),'constant')\n",
    "                    else:\n",
    "                        p_ar = ar[i-arraylength:i]\n",
    "                    past_validate.append(p_ar)\n",
    "                    future_validate.append(ar[i+j:i+j+1])\n",
    "                    #future_validate.append(ar[i:i+futurelag])\n",
    "    return np.asarray(past_train), np.asarray(future_train), np.asarray(past_validate), np.asarray(future_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformDataByCols(data, pastlag, targetCol, ExoCols = [], futurelag = 1, validation_span = 16, arraylength = None):\n",
    "\n",
    "    if arraylength is None:\n",
    "        arraylength = pastlag\n",
    "    past_train = []\n",
    "    future_train  = []\n",
    "    past_validate = []\n",
    "    future_validate  = []\n",
    "    \n",
    "    cols = [targetCol] + ExoCols\n",
    "    for c in cols: \n",
    "        p_t_c =  []\n",
    "        f_t_c = []\n",
    "        p_v_c = []\n",
    "        f_v_c  = []\n",
    "        ar = np.asarray(data[c])\n",
    "        l = len(ar)\n",
    "        iv = (l-futurelag) - validation_span\n",
    "        #iv = pastlag + int(((l-futurelag)-pastlag)*(1-validation_percentage))\n",
    "        for i in range(pastlag,len(ar)-futurelag+1):\n",
    "            if i <= iv:\n",
    "                if i-arraylength < 0:\n",
    "                        p_ar = ar[0:i]\n",
    "                        p_ar = np.pad(p_ar,(arraylength - len(p_ar),0),'constant')\n",
    "                else:\n",
    "                        p_ar = ar[i-arraylength:i]\n",
    "                p_t_c.append(p_ar)\n",
    "                f_t_c.append(ar[i:i+futurelag])\n",
    "            else:\n",
    "                if i-arraylength < 0:\n",
    "                        p_ar = ar[0:i]\n",
    "                        p_ar = np.pad(p_ar,(arraylength - len(p_ar),0),'constant')\n",
    "                else:\n",
    "                        p_ar = ar[i-arraylength:i]\n",
    "                p_v_c.append(p_ar)\n",
    "                f_v_c.append(ar[i:i+futurelag])\n",
    "        \n",
    "        past_train.append(np.expand_dims(np.asarray(p_t_c),-1))\n",
    "        past_validate.append(np.expand_dims(np.asarray(p_v_c),-1))\n",
    "        if  c == targetCol:\n",
    "            future_train = np.asarray(f_t_c)\n",
    "            future_validate = np.asarray(f_v_c)\n",
    "                \n",
    "                \n",
    "    return past_train, future_train, past_validate, future_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "def RMSEperLag(x_true,x_pred):\n",
    "    return np.sqrt(np.mean((x_true - x_pred)**2,axis= 0))\n",
    "\n",
    "\n",
    "\n",
    "class IntervalEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=10):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        \n",
    "    def on_epoch_start(self,epoch, logs={}):\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            \n",
    "            \n",
    "            score = RMSEperLag(self.y_val, y_pred)\n",
    "            \n",
    "            for s in score:\n",
    "                print(s)\n",
    "            #print(\"interval evaluation - epoch: {:d} - score: {:.6f}\".format(epoch, score))\n",
    "\n",
    "    \n",
    "class LRRestart(Callback):\n",
    "    def __init__(self, maxLR, maxEpoch, patience, minLR = 0.1e-5):\n",
    "        super(Callback, self).__init__()\n",
    "        self.maxLR = maxLR    \n",
    "        self.patience = patience\n",
    "        self.minLR = minLR\n",
    "        self.restart = True\n",
    "        self.lastRestartEpoch = 1\n",
    "        self.best_val_loss = None\n",
    "        self.wait = 0 \n",
    "        \n",
    "    def schedule(epoch):\n",
    "        reductionRate = ((self.maxLR - self.MinLR)/self.maxEpoch) / self.maxLR\n",
    "        lr = self.maxLR - (epoch - lastRestartEpoch) *max(reductionRate*self.maxLR, self.minLR)\n",
    "        return lr\n",
    "    \n",
    "    def on_training_begin(self, logs ={}):\n",
    "        \n",
    "        self.wait = 0\n",
    "        \n",
    "    def on_epoch_start(self,epoch, logs=None):\n",
    "            \n",
    "        if self.restart or (epoch - self.lastRestartEpoch) > self.maxEpoch:\n",
    "            self.lastRestartEpoch = epoch\n",
    "            self.restart = False\n",
    "        lr = schedule(epoch)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "    def on_epoch_end(self, epoch,logs = None):\n",
    "            \n",
    "            logs = logs or {}\n",
    "            logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
    "            \n",
    "            self.current_val_loss = logs.get('val_loss')\n",
    "            \n",
    "            if self.best_val_loss is None:\n",
    "                self.best_val_loss = self.current_val_loss\n",
    "            \n",
    "            if self.current_val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = self.current_val_loss \n",
    "                self.wait = 0\n",
    "            else:\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.patience:\n",
    "                    self.restart = True\n",
    "                    #self.model.stop_training = True\n",
    "                    \n",
    "class UnfreezeLayer(Callback):\n",
    "    def __init__(self, layerNames = [], unfreezeSchedule = []):\n",
    "        super(Callback, self).__init__()\n",
    "        self.layerNames = layerNames    \n",
    "        self.unfreezeSchedule = unfreezeSchedule\n",
    "        \n",
    "    def on_epoch_start(self,epoch, logs=None):\n",
    "        \n",
    "        for i, u in enumerate(self.unfreezeSchedule):\n",
    "            if u == epoch:\n",
    "                for ln in  [l.name for l in g_lstm.layers if layerNames[u] + '.' in l.name ]:\n",
    "                    self.model.get_layer(ln).trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "\n",
    "def PermaDropout(rate):\n",
    "    return Lambda(lambda x: K.dropout(x, level=rate))\n",
    "\n",
    "def expand_dims(x):\n",
    "    return K.expand_dims(x,1)\n",
    "def expand_dims_output_shape(input_shape):\n",
    "    return (input_shape[0],1,input_shape[1])\n",
    "\n",
    "def expand_dims2(x):\n",
    "    return K.expand_dims(x,2)\n",
    "def expand_dims_output_shape2(input_shape):\n",
    "    return (input_shape[0],1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_t, future_t, past_v, future_v  = transformData(data,pastlag = 90, futurelag = 1, arraylength   = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General LSTM Model\n",
    "\n",
    "- define Model\n",
    "- Train Model on all Data \n",
    "- save trained weights\n",
    "\n",
    "\n",
    "layer_conv_1d(filters=64, kernel_size=4, activation=\"relu\", input_shape=c(lookback, dim(dm)[[-1]])) %>%\n",
    "  layer_max_pooling_1d(pool_size=4) %>%\n",
    "  layer_flatten() %>%\n",
    "  layer_dense(units=lookback * dim(dm)[[-1]], activation=\"relu\") %>%\n",
    "  layer_dropout(rate=0.2) %>%\n",
    "  layer_dense(units=1, activation=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-539016ff95e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#g_lstm.add(Dropout(0.2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m#g_lstm.add(Dense(30, name = 'glstm_dense'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mg_lstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\Time\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\Time\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m                                          \u001b[1;34m'You can build it manually via: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m--> 431\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\Time\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    864\u001b[0m                                       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'kernel'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m                                       \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                                       constraint=self.kernel_constraint)\n\u001b[0m\u001b[0;32m    867\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m             self.bias = self.add_weight(shape=(self.units,),\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\Time\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\Time\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         weight = K.variable(initializer(shape),\n\u001b[0m\u001b[0;32m    250\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\Time\\lib\\site-packages\\keras\\initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, shape, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mscale\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfan_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[0mscale\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfan_in\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfan_out\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'normal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;31m# 0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "#### DECODER MODEL\n",
    "decoder = Sequential()\n",
    "dec_input = Input((90,1))\n",
    "#dec_batchNorm = BatchNormalization()(dec_input)\n",
    "dec_batchNorm = dec_input\n",
    "### Conv\n",
    "\n",
    "dec_conv = Conv1D(8,3,activation=\"relu\")(dec_batchNorm)\n",
    "dec_pool = MaxPooling1D(2)(dec_conv)\n",
    "dec_conv2 = Conv1D(4,5,activation=\"relu\")(dec_pool)\n",
    "dec_pool2 = MaxPooling1D(4)(dec_conv)\n",
    "#dec_pool2 = SeqSelfAttention(attention_activation='sigmoid')(dec_pool2) \n",
    "#dec_lstm_conv = LSTM(10,dropout = 0.5 ,return_sequences = False)(dec_pool2)\n",
    "dec_conv_flat = Flatten()(dec_pool2)\n",
    "#dec_conv_flat = Lambda(lambda x: K.batch_flatten(x))(dec_pool2)\n",
    "dec_conv_flat = Dropout(0.2)(dec_conv_flat)\n",
    "#dec_conv_flat = dec_lstm_conv\n",
    "\n",
    "### LSTM \n",
    "#dec_attention = SeqSelfAttention(attention_activation='sigmoid')(dec_batchNorm) \n",
    "dec_lstm = LSTM(10,dropout = 0.5 ,return_sequences = False)(dec_batchNorm)\n",
    "#dec_attention = SeqSelfAttention(attention_activation='sigmoid')(dec_lstm) \n",
    "#dec_lstm_flat = Flatten()(dec_attention)\n",
    "dec_lstm_flat = dec_lstm\n",
    "\n",
    "\n",
    "dec_output = concatenate([dec_conv_flat, dec_lstm_flat],axis = -1)\n",
    "#dec_output = dec_lstm_flat\n",
    "#dec_output = dec_conv_flat\n",
    "\n",
    "decoder = Model(dec_input,dec_output)\n",
    "\n",
    "g_lstm = Sequential()\n",
    "g_lstm.add(decoder)\n",
    "#g_lstm.add(Dense(90*4, activation=\"relu\"))\n",
    "#g_lstm.add(Dropout(0.2))\n",
    "#g_lstm.add(Dense(30, name = 'glstm_dense'))\n",
    "g_lstm.add(Dense(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_20 (Model)             (None, 186)               512       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 187       \n",
      "=================================================================\n",
      "Total params: 699\n",
      "Trainable params: 699\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "g_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1450 samples, validate on 160 samples\n",
      "Epoch 1/100\n",
      "1450/1450 [==============================] - 9s 6ms/step - loss: 0.0741 - val_loss: 0.0528\n",
      "Epoch 2/100\n",
      "1450/1450 [==============================] - 1s 373us/step - loss: 0.0642 - val_loss: 0.0494\n",
      "Epoch 3/100\n",
      "1450/1450 [==============================] - 1s 379us/step - loss: 0.0561 - val_loss: 0.0538\n",
      "Epoch 4/100\n",
      "1450/1450 [==============================] - 1s 382us/step - loss: 0.0525 - val_loss: 0.0441\n",
      "Epoch 5/100\n",
      "1450/1450 [==============================] - 1s 394us/step - loss: 0.0499 - val_loss: 0.0357\n",
      "Epoch 6/100\n",
      "1450/1450 [==============================] - 1s 346us/step - loss: 0.0419 - val_loss: 0.0368\n",
      "Epoch 7/100\n",
      "1450/1450 [==============================] - 1s 369us/step - loss: 0.0425 - val_loss: 0.0336\n",
      "Epoch 8/100\n",
      "1450/1450 [==============================] - 1s 373us/step - loss: 0.0375 - val_loss: 0.0282\n",
      "Epoch 9/100\n",
      "1450/1450 [==============================] - 1s 431us/step - loss: 0.0378 - val_loss: 0.0271\n",
      "Epoch 10/100\n",
      "1450/1450 [==============================] - 1s 435us/step - loss: 0.0329 - val_loss: 0.0241\n",
      "Epoch 11/100\n",
      "1450/1450 [==============================] - 1s 390us/step - loss: 0.0336 - val_loss: 0.0255\n",
      "Epoch 12/100\n",
      "1450/1450 [==============================] - 1s 446us/step - loss: 0.0330 - val_loss: 0.0237\n",
      "Epoch 13/100\n",
      "1450/1450 [==============================] - 1s 515us/step - loss: 0.0305 - val_loss: 0.0228\n",
      "Epoch 14/100\n",
      "1450/1450 [==============================] - 1s 444us/step - loss: 0.0298 - val_loss: 0.0223\n",
      "Epoch 15/100\n",
      "1450/1450 [==============================] - 1s 461us/step - loss: 0.0281 - val_loss: 0.0217\n",
      "Epoch 16/100\n",
      "1450/1450 [==============================] - 1s 585us/step - loss: 0.0249 - val_loss: 0.0196\n",
      "Epoch 17/100\n",
      "1450/1450 [==============================] - 1s 476us/step - loss: 0.0257 - val_loss: 0.0185\n",
      "Epoch 18/100\n",
      "1450/1450 [==============================] - 1s 454us/step - loss: 0.0257 - val_loss: 0.0184\n",
      "Epoch 19/100\n",
      "1450/1450 [==============================] - 1s 623us/step - loss: 0.0235 - val_loss: 0.0190\n",
      "Epoch 20/100\n",
      "1450/1450 [==============================] - 1s 467us/step - loss: 0.0226 - val_loss: 0.0178\n",
      "Epoch 21/100\n",
      "1450/1450 [==============================] - 1s 475us/step - loss: 0.0217 - val_loss: 0.0156\n",
      "Epoch 22/100\n",
      "1450/1450 [==============================] - 1s 421us/step - loss: 0.0209 - val_loss: 0.0180\n",
      "Epoch 23/100\n",
      "1450/1450 [==============================] - 1s 384us/step - loss: 0.0191 - val_loss: 0.0177\n",
      "Epoch 24/100\n",
      "1450/1450 [==============================] - 1s 363us/step - loss: 0.0207 - val_loss: 0.0178\n",
      "Epoch 25/100\n",
      "1450/1450 [==============================] - 1s 348us/step - loss: 0.0180 - val_loss: 0.0168\n",
      "Epoch 26/100\n",
      "1450/1450 [==============================] - 1s 497us/step - loss: 0.0167 - val_loss: 0.0163\n",
      "Epoch 27/100\n",
      "1450/1450 [==============================] - 1s 531us/step - loss: 0.0178 - val_loss: 0.0180\n",
      "Epoch 28/100\n",
      "1450/1450 [==============================] - 1s 456us/step - loss: 0.0169 - val_loss: 0.0165\n",
      "Epoch 29/100\n",
      "1450/1450 [==============================] - 1s 571us/step - loss: 0.0153 - val_loss: 0.0183\n",
      "Epoch 30/100\n",
      "1450/1450 [==============================] - 1s 392us/step - loss: 0.0157 - val_loss: 0.0181\n",
      "Epoch 31/100\n",
      "1450/1450 [==============================] - 1s 555us/step - loss: 0.0150 - val_loss: 0.0178\n",
      "Epoch 32/100\n",
      "1450/1450 [==============================] - 1s 386us/step - loss: 0.0149 - val_loss: 0.0193\n",
      "Epoch 33/100\n",
      "1450/1450 [==============================] - 1s 386us/step - loss: 0.0144 - val_loss: 0.0200\n",
      "Epoch 34/100\n",
      "1450/1450 [==============================] - 1s 364us/step - loss: 0.0142 - val_loss: 0.0165\n",
      "Epoch 35/100\n",
      "1450/1450 [==============================] - 1s 384us/step - loss: 0.0136 - val_loss: 0.0187\n",
      "Epoch 36/100\n",
      "1450/1450 [==============================] - 1s 385us/step - loss: 0.0138 - val_loss: 0.0188\n",
      "Epoch 37/100\n",
      "1450/1450 [==============================] - 1s 394us/step - loss: 0.0127 - val_loss: 0.0147\n",
      "Epoch 38/100\n",
      "1450/1450 [==============================] - 1s 355us/step - loss: 0.0136 - val_loss: 0.0182\n",
      "Epoch 39/100\n",
      "1450/1450 [==============================] - 1s 379us/step - loss: 0.0132 - val_loss: 0.0182\n",
      "Epoch 40/100\n",
      "1450/1450 [==============================] - 1s 394us/step - loss: 0.0127 - val_loss: 0.0179\n",
      "Epoch 41/100\n",
      "1450/1450 [==============================] - 1s 348us/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 42/100\n",
      "1450/1450 [==============================] - 0s 340us/step - loss: 0.0119 - val_loss: 0.0175\n",
      "Epoch 43/100\n",
      "1450/1450 [==============================] - 1s 357us/step - loss: 0.0119 - val_loss: 0.0188\n",
      "Epoch 44/100\n",
      "1450/1450 [==============================] - 1s 370us/step - loss: 0.0114 - val_loss: 0.0171\n",
      "Epoch 45/100\n",
      "1450/1450 [==============================] - 1s 345us/step - loss: 0.0106 - val_loss: 0.0185\n",
      "Epoch 46/100\n",
      "1450/1450 [==============================] - 1s 361us/step - loss: 0.0111 - val_loss: 0.0150\n",
      "Epoch 47/100\n",
      "1450/1450 [==============================] - 1s 353us/step - loss: 0.0110 - val_loss: 0.0149\n",
      "Epoch 48/100\n",
      "1450/1450 [==============================] - 1s 361us/step - loss: 0.0113 - val_loss: 0.0189\n",
      "Epoch 49/100\n",
      "1450/1450 [==============================] - 1s 369us/step - loss: 0.0104 - val_loss: 0.0148\n",
      "Epoch 50/100\n",
      "1450/1450 [==============================] - 1s 393us/step - loss: 0.0106 - val_loss: 0.0163\n",
      "Epoch 51/100\n",
      "1450/1450 [==============================] - 1s 386us/step - loss: 0.0106 - val_loss: 0.0143\n",
      "Epoch 52/100\n",
      "1450/1450 [==============================] - 1s 353us/step - loss: 0.0102 - val_loss: 0.0165\n",
      "Epoch 53/100\n",
      "1450/1450 [==============================] - 1s 360us/step - loss: 0.0103 - val_loss: 0.0143\n",
      "Epoch 54/100\n",
      "1450/1450 [==============================] - 1s 357us/step - loss: 0.0101 - val_loss: 0.0164\n",
      "Epoch 55/100\n",
      "1450/1450 [==============================] - 1s 357us/step - loss: 0.0099 - val_loss: 0.0141\n",
      "Epoch 56/100\n",
      "1450/1450 [==============================] - 0s 342us/step - loss: 0.0096 - val_loss: 0.0154\n",
      "Epoch 57/100\n",
      "1450/1450 [==============================] - 1s 358us/step - loss: 0.0097 - val_loss: 0.0119\n",
      "Epoch 58/100\n",
      "1450/1450 [==============================] - 1s 369us/step - loss: 0.0099 - val_loss: 0.0141\n",
      "Epoch 59/100\n",
      "1450/1450 [==============================] - 1s 447us/step - loss: 0.0097 - val_loss: 0.0147\n",
      "Epoch 60/100\n",
      "1450/1450 [==============================] - 1s 394us/step - loss: 0.0099 - val_loss: 0.0153\n",
      "Epoch 61/100\n",
      "1450/1450 [==============================] - 1s 391us/step - loss: 0.0101 - val_loss: 0.0115\n",
      "Epoch 62/100\n",
      "1450/1450 [==============================] - 1s 361us/step - loss: 0.0096 - val_loss: 0.0158\n",
      "Epoch 63/100\n",
      "1450/1450 [==============================] - 1s 364us/step - loss: 0.0091 - val_loss: 0.0139\n",
      "Epoch 64/100\n",
      "1450/1450 [==============================] - 0s 337us/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 65/100\n",
      "1450/1450 [==============================] - 1s 371us/step - loss: 0.0090 - val_loss: 0.0159\n",
      "Epoch 66/100\n",
      "1450/1450 [==============================] - 1s 370us/step - loss: 0.0091 - val_loss: 0.0104\n",
      "Epoch 67/100\n",
      "1450/1450 [==============================] - 0s 339us/step - loss: 0.0090 - val_loss: 0.0143\n",
      "Epoch 68/100\n",
      "1450/1450 [==============================] - 1s 347us/step - loss: 0.0087 - val_loss: 0.0128\n",
      "Epoch 69/100\n",
      "1450/1450 [==============================] - 1s 362us/step - loss: 0.0087 - val_loss: 0.0129\n",
      "Epoch 70/100\n",
      "1450/1450 [==============================] - 1s 364us/step - loss: 0.0086 - val_loss: 0.0095\n",
      "Epoch 71/100\n",
      "1450/1450 [==============================] - 0s 338us/step - loss: 0.0093 - val_loss: 0.0146\n",
      "Epoch 72/100\n",
      "1450/1450 [==============================] - 1s 365us/step - loss: 0.0081 - val_loss: 0.0114\n",
      "Epoch 73/100\n",
      "1450/1450 [==============================] - 1s 349us/step - loss: 0.0086 - val_loss: 0.0118\n",
      "Epoch 74/100\n",
      "1450/1450 [==============================] - 1s 380us/step - loss: 0.0085 - val_loss: 0.0142\n",
      "Epoch 75/100\n",
      "1450/1450 [==============================] - 1s 347us/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 76/100\n",
      "1450/1450 [==============================] - 1s 364us/step - loss: 0.0084 - val_loss: 0.0120\n",
      "Epoch 77/100\n",
      "1450/1450 [==============================] - 1s 370us/step - loss: 0.0085 - val_loss: 0.0117\n",
      "Epoch 78/100\n",
      "1450/1450 [==============================] - 0s 321us/step - loss: 0.0081 - val_loss: 0.0101\n",
      "Epoch 79/100\n",
      "1450/1450 [==============================] - 1s 350us/step - loss: 0.0083 - val_loss: 0.0102\n",
      "Epoch 80/100\n",
      "1450/1450 [==============================] - 1s 372us/step - loss: 0.0077 - val_loss: 0.0111\n",
      "Epoch 81/100\n",
      "1450/1450 [==============================] - 1s 350us/step - loss: 0.0078 - val_loss: 0.0096\n",
      "Epoch 82/100\n",
      "1450/1450 [==============================] - 0s 342us/step - loss: 0.0077 - val_loss: 0.0100\n",
      "Epoch 83/100\n",
      "1450/1450 [==============================] - 1s 366us/step - loss: 0.0081 - val_loss: 0.0105\n",
      "Epoch 84/100\n",
      "1450/1450 [==============================] - 1s 356us/step - loss: 0.0079 - val_loss: 0.0085\n",
      "Epoch 85/100\n",
      "1450/1450 [==============================] - 0s 328us/step - loss: 0.0083 - val_loss: 0.0116\n",
      "Epoch 86/100\n",
      "1450/1450 [==============================] - 1s 348us/step - loss: 0.0076 - val_loss: 0.0096\n",
      "Epoch 87/100\n",
      "1450/1450 [==============================] - 0s 340us/step - loss: 0.0078 - val_loss: 0.0109\n",
      "Epoch 88/100\n",
      "1450/1450 [==============================] - 1s 349us/step - loss: 0.0082 - val_loss: 0.0105\n",
      "Epoch 89/100\n",
      "1450/1450 [==============================] - 0s 331us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 90/100\n",
      "1450/1450 [==============================] - 1s 348us/step - loss: 0.0081 - val_loss: 0.0124\n",
      "Epoch 91/100\n",
      "1450/1450 [==============================] - 0s 342us/step - loss: 0.0076 - val_loss: 0.0072\n",
      "Epoch 92/100\n",
      "1450/1450 [==============================] - 0s 342us/step - loss: 0.0073 - val_loss: 0.0069\n",
      "Epoch 93/100\n",
      "1450/1450 [==============================] - 0s 321us/step - loss: 0.0075 - val_loss: 0.0101\n",
      "Epoch 94/100\n",
      "1450/1450 [==============================] - 0s 331us/step - loss: 0.0072 - val_loss: 0.0065\n",
      "Epoch 95/100\n",
      "1450/1450 [==============================] - 0s 343us/step - loss: 0.0079 - val_loss: 0.0106\n",
      "Epoch 96/100\n",
      "1450/1450 [==============================] - 0s 322us/step - loss: 0.0076 - val_loss: 0.0092\n",
      "Epoch 97/100\n",
      "1450/1450 [==============================] - 0s 336us/step - loss: 0.0073 - val_loss: 0.0080\n",
      "Epoch 98/100\n",
      "1450/1450 [==============================] - 0s 330us/step - loss: 0.0074 - val_loss: 0.0094\n",
      "Epoch 99/100\n",
      "1450/1450 [==============================] - 0s 332us/step - loss: 0.0071 - val_loss: 0.0080\n",
      "Epoch 100/100\n",
      "1450/1450 [==============================] - 1s 360us/step - loss: 0.0072 - val_loss: 0.0076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21f8de7fdd8>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "g_lstm.compile(loss = 'mse',optimizer = 'adam')\n",
    "ival = IntervalEvaluation(validation_data =(np.expand_dims(past_v,-1),future_v), interval=1)\n",
    "lrre = LRRestart(maxLR = .5, maxEpoch = 10, patience = 5)\n",
    "\n",
    "g_lstm.fit(np.expand_dims(past_t,-1), future_t, batch_size = 200, epochs=100, validation_data = (np.expand_dims(past_v,-1),future_v), \n",
    "           #callbacks=[lrre],\n",
    "           verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lstm.save('glstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predA = decoder.predict(np.expand_dims(past_t,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate with Exogenen Variables\n",
    "\n",
    "- Embedding of  each TS with general  LSTM\n",
    "- Multi Attettion Head over all TS\n",
    "- Estimate Full future timespan\n",
    "\n",
    "Train Model first with froozen LSTM Layer... unfreeze later in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EXO = [\"BAC\", \"MU\",\"AKS\",\"CLF\",\"SKX\",\"PBR\",\"NFX\",\"MS\",\"CTL\"]\n",
    "past_t, future_t, past_v, future_v  = transformDataByCols(data,targetCol = \"ET\", ExoCols = EXO, pastlag = 90, futurelag = 10, arraylength =90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs,TIME_STEPS, SINGLE_ATTENTION_VECTOR ):\n",
    "\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "\n",
    "    input_dim =186# int(inputs.shape[2])\n",
    "\n",
    "    a = Permute((2, 1))(inputs)\n",
    "\n",
    "    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
    "\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])#, name='attention_mul', mode='mul')\n",
    "\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The shape of the input to \"Flatten\" is not fully defined (got (10, None). Make sure to pass a complete \"input_shape\" or \"batch_input_shape\" argument to the first layer in your model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-3291e7e445df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#out = SeqSelfAttention(attention_activation='sigmoid')(out_dec)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mo2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_dec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mo2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mo2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\Time\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    472\u001b[0m             if all([s is not None\n\u001b[0;32m    473\u001b[0m                     for s in to_list(input_shape)]):\n\u001b[1;32m--> 474\u001b[1;33m                 \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\Time\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    498\u001b[0m             raise ValueError('The shape of the input to \"Flatten\" '\n\u001b[0;32m    499\u001b[0m                              \u001b[1;34m'is not fully defined '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m                              \u001b[1;34m'(got '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m                              \u001b[1;34m'Make sure to pass a complete \"input_shape\" '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m                              \u001b[1;34m'or \"batch_input_shape\" argument to the first '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The shape of the input to \"Flatten\" is not fully defined (got (10, None). Make sure to pass a complete \"input_shape\" or \"batch_input_shape\" argument to the first layer in your model."
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense ,LSTM,concatenate\n",
    "\n",
    "STEPS = 2\n",
    "\n",
    "\n",
    "in_in = []\n",
    "out_in = []\n",
    "\n",
    "for i in range(len(EXO)+1):\n",
    "    \n",
    "    \n",
    "    inp = Input((90,1))\n",
    "    in_in.append(inp)\n",
    "        \n",
    "    inpp = inp \n",
    "    out_pp = []\n",
    "    for i in range(STEPS):\n",
    "        out_temp = g_lstm(inpp)\n",
    "        \n",
    "        inpp = concatenate ([inpp[:, 1:],Lambda(expand_dims2, expand_dims_output_shape2)(out_temp)], axis=1)\n",
    "        \n",
    "        out_pp.append(out_temp)\n",
    "        \n",
    "    a = concatenate(out_pp,axis=-1)\n",
    "    a= Lambda(expand_dims, expand_dims_output_shape)(a)\n",
    "    out_in.append(a)\n",
    "        \n",
    "out_dec=concatenate(out_in,axis=1)   \n",
    "#out = SeqSelfAttention(attention_activation='sigmoid')(out_dec)\n",
    "    \n",
    "o2 = Flatten()(out_dec)\n",
    "o2 = Dense(STEPS)(o2)\n",
    "model = Model(inputs = in_in, outputs = [o2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense ,LSTM,concatenate\n",
    "\n",
    "\n",
    "in_in = []\n",
    "out_in = []\n",
    "\n",
    "#bb = BatchNormalization()\n",
    "\n",
    "\n",
    "decoder.trainable = False\n",
    "\n",
    "#a_layer = SeqSelfAttention(attention_activation='sigmoid', name = 'att.0', weights = glstm_a_weights)\n",
    "#a_layer.trainable = False\n",
    "for i in range(len(EXO)+1):\n",
    "    inp = Input((90,1))\n",
    "    in_in.append(inp)  \n",
    "    a = decoder(inp)\n",
    "    #a = concatenate([a,Flatten()(inp)],axis=-1)\n",
    "    a= Lambda(expand_dims, expand_dims_output_shape)(a)\n",
    "    out_in.append(a)\n",
    "        \n",
    "\n",
    "if len(EXO) >= 1:\n",
    "    out_dec=concatenate(out_in,axis=1)\n",
    "else:\n",
    "    out_dec = out_in\n",
    "    \n",
    "#out_dec = BatchNormalization()(out_dec)\n",
    "out_ar = []    \n",
    "\n",
    "#out_dec = MultiHeadAttention(head_num=5)(out_dec)\n",
    "\n",
    "#out = attention_3d_block(out_dec, len(EXO) + 1, False)\n",
    "out = SeqSelfAttention(attention_activation='sigmoid')(out_dec)\n",
    "'''\n",
    "for  j in range(16):\n",
    "    #out = attention_3d_block(out_dec, len(EXO) + 1, True)\n",
    "    #out = SeqSelfAttention(attention_activation='sigmoid')(out_dec)\n",
    "\n",
    "    #out = MultiHeadAttention(\n",
    "    #    head_num=5)(out_dec)\n",
    "    out= Dropout(0.2)(out_dec)\n",
    "    out = Flatten()(out)\n",
    "    out = Dense(20, activation = \"relu\")(out)\n",
    "    out= Dropout(0.2)(out)\n",
    "\n",
    "#out = Dense(30)(out)\n",
    "    out = Dense(1)(out)\n",
    "    out_ar.append(out)\n",
    "    \n",
    "oo = concatenate(out_ar,axis = -1)\n",
    "'''\n",
    "#o2 = attention_3d_block(out_dec, len(EXO), False)\n",
    "o2 = Flatten()(out_dec)\n",
    "o2 = Dense(10)(o2)\n",
    "#ooo = Add()([oo,o2])\n",
    "model = Model(inputs = in_in, outputs = [o2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse(yTrue,yPred):\n",
    "    ones = K.ones_like(yTrue[0,:])\n",
    "    idx = K.cumsum(ones)\n",
    "    \n",
    "    return K.mean((1/idx)*K.square(yTrue- yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 136 samples, validate on 16 samples\n",
      "Epoch 1/100\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.0149 - val_loss: 0.0131\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0102 - val_loss: 0.0119\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.0072\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0076 - val_loss: 0.0087\n",
      "Epoch 8/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0077 - val_loss: 0.0141\n",
      "Epoch 9/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0079 - val_loss: 0.0071\n",
      "Epoch 10/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0085 - val_loss: 0.0106\n",
      "Epoch 11/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0082 - val_loss: 0.0132\n",
      "Epoch 12/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0085 - val_loss: 0.0065\n",
      "Epoch 13/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0077 - val_loss: 0.0100\n",
      "Epoch 14/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0075 - val_loss: 0.0096\n",
      "Epoch 16/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0078 - val_loss: 0.0113\n",
      "Epoch 17/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0076 - val_loss: 0.0054\n",
      "Epoch 18/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074 - val_loss: 0.0092\n",
      "Epoch 19/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0079 - val_loss: 0.0061\n",
      "Epoch 20/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0077 - val_loss: 0.0090\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0079 - val_loss: 0.0107\n",
      "Epoch 22/100\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 0.0083 - val_loss: 0.0090\n",
      "Epoch 23/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.0086\n",
      "Epoch 24/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0077 - val_loss: 0.0125\n",
      "Epoch 25/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0082 - val_loss: 0.0073\n",
      "Epoch 26/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0072 - val_loss: 0.0126\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0062\n",
      "Epoch 28/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0075 - val_loss: 0.0100\n",
      "Epoch 29/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0074\n",
      "Epoch 30/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0068 - val_loss: 0.0077\n",
      "Epoch 31/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0083 - val_loss: 0.0130\n",
      "Epoch 32/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 33/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0098\n",
      "Epoch 34/100\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 0.0072 - val_loss: 0.0116\n",
      "Epoch 35/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 36/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074 - val_loss: 0.0086\n",
      "Epoch 37/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0117\n",
      "Epoch 38/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0078 - val_loss: 0.0064\n",
      "Epoch 39/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0092 - val_loss: 0.0182\n",
      "Epoch 40/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0092 - val_loss: 0.0080\n",
      "Epoch 41/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0076 - val_loss: 0.0091\n",
      "Epoch 42/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074 - val_loss: 0.0081\n",
      "Epoch 43/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0093\n",
      "Epoch 44/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0082 - val_loss: 0.0083\n",
      "Epoch 45/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0092\n",
      "Epoch 46/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073 - val_loss: 0.0090\n",
      "Epoch 47/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0072 - val_loss: 0.0171\n",
      "Epoch 48/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0079 - val_loss: 0.0081\n",
      "Epoch 49/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073 - val_loss: 0.0082\n",
      "Epoch 50/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074 - val_loss: 0.0105\n",
      "Epoch 51/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 52/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074 - val_loss: 0.0083\n",
      "Epoch 53/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0070 - val_loss: 0.0096\n",
      "Epoch 54/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0072 - val_loss: 0.0068\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0078\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0068 - val_loss: 0.0056\n",
      "Epoch 57/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0078 - val_loss: 0.0159\n",
      "Epoch 58/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0078 - val_loss: 0.0073\n",
      "Epoch 59/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074 - val_loss: 0.0148\n",
      "Epoch 60/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0083 - val_loss: 0.0092\n",
      "Epoch 61/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0067 - val_loss: 0.0065\n",
      "Epoch 62/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0064 - val_loss: 0.0078\n",
      "Epoch 63/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0068 - val_loss: 0.0113\n",
      "Epoch 64/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0089\n",
      "Epoch 65/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0107\n",
      "Epoch 66/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0084\n",
      "Epoch 67/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0062 - val_loss: 0.0102\n",
      "Epoch 68/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0074 - val_loss: 0.0118\n",
      "Epoch 69/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 70/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0077 - val_loss: 0.0091\n",
      "Epoch 71/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0124\n",
      "Epoch 72/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0070 - val_loss: 0.0116\n",
      "Epoch 73/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0077 - val_loss: 0.0195\n",
      "Epoch 74/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0086 - val_loss: 0.0077\n",
      "Epoch 75/100\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 0.0074 - val_loss: 0.0113\n",
      "Epoch 76/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0078 - val_loss: 0.0199\n",
      "Epoch 77/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0081 - val_loss: 0.0078\n",
      "Epoch 78/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0084 - val_loss: 0.0124\n",
      "Epoch 79/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0149\n",
      "Epoch 80/100\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 0.0072 - val_loss: 0.0067\n",
      "Epoch 81/100\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 0.0075 - val_loss: 0.0104\n",
      "Epoch 82/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0075 - val_loss: 0.0145\n",
      "Epoch 83/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073 - val_loss: 0.0066\n",
      "Epoch 84/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0140\n",
      "Epoch 85/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0067 - val_loss: 0.0107\n",
      "Epoch 86/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0068 - val_loss: 0.0153\n",
      "Epoch 87/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0068 - val_loss: 0.0084\n",
      "Epoch 88/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0068 - val_loss: 0.0109\n",
      "Epoch 89/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 90/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0124\n",
      "Epoch 91/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 92/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0068 - val_loss: 0.0144\n",
      "Epoch 93/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 94/100\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.0081\n",
      "Epoch 95/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0068 - val_loss: 0.0113\n",
      "Epoch 96/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0068 - val_loss: 0.0106\n",
      "Epoch 97/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0067 - val_loss: 0.0099\n",
      "Epoch 98/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0142\n",
      "Epoch 99/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0084\n",
      "Epoch 100/100\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 0.0073 - val_loss: 0.0125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21f830bb780>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'mse',optimizer = 'adam')\n",
    "\n",
    "#ival = IntervalEvaluation(validation_data =(past_v,future_v), interval=1)\n",
    "lrre = LRRestart(maxLR = .5, maxEpoch = 10, patience = 5)\n",
    "unfreeze = UnfreezeLayer(['lstm'], [20])\n",
    "\n",
    "model.fit(past_t, future_t, batch_size = 32, epochs=100, validation_data = (past_v,future_v))#, callbacks = [lrre,unfreeze])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 10 array(s), but instead got the following list of 1 arrays: [array([[0.35294119, 0.33285513, 0.29555237, ..., 0.07030133, 0.10186514,\n        0.10616934],\n       [0.35294119, 0.33285513, 0.29555237, ..., 0.07030133, 0.10186514,\n        0.10616934],\n       [0.3...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-5715b0953673>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpast_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\Time\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\Time\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\Time\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 10 array(s), but instead got the following list of 1 arrays: [array([[0.35294119, 0.33285513, 0.29555237, ..., 0.07030133, 0.10186514,\n        0.10616934],\n       [0.35294119, 0.33285513, 0.29555237, ..., 0.07030133, 0.10186514,\n        0.10616934],\n       [0.3..."
     ]
    }
   ],
   "source": [
    "predA = model.predict(past_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-dd68d438a228>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpast\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpast_v\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfuture_v\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mpast_x\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mfuture_x\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "I =-1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "past= np.squeeze(past_v[0][I])\n",
    "future= future_v[I]\n",
    "past_x =[i for i in range(len(past))]\n",
    "future_x =[i for i in range(len(past),len(past)+len(future))]\n",
    "pred = predA[I]\n",
    "\n",
    "\n",
    "#pop_india = [449.48, 553.57, 696.783, 870.133, 1000.4, 1309.1]\n",
    "plt.plot(past_x, past, color='blue')\n",
    "plt.plot(future_x, future, color='g')\n",
    "plt.plot(future_x, pred, color='orange')\n",
    "plt.xlabel('Countries')\n",
    "plt.ylabel('Population in million')\n",
    "plt.title('Pakistan India Population till 2010')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_t, future_t, past_v, future_v  = transformDataByCols(data,targetCol = \"IWM\", ExoCols = [], pastlag = 30)\n",
    "t_lstm = Sequential()\n",
    "\n",
    "t_lstm.add(LSTM(20, dropout=0.2, name = 'glstm'))\n",
    "t_lstm.add(Dense(1, name = 'glstm_dense'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_lstm.compile(loss = 'mse',optimizer = 'adam')\n",
    "t_lstm.fit(past_t, future_t, batch_size = 10, epochs=100, validation_data = (past_v,future_v), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
